{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hyperbolic_nn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLMTdd5m49Eq",
        "colab_type": "text"
      },
      "source": [
        "### Environment Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-0rHFza5Nkl",
        "colab_type": "text"
      },
      "source": [
        "##### GPU config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_d-kNkHa4-px",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Runtime -> Change runtime type -> Hardware Accelorator (change from None to GPU)\n",
        "Make sure there is no assertion error below\n",
        "\"\"\"\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "assert tf.test.gpu_device_name() != '', \"NO GPU DETECTED\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GLUoFdTf5QKM",
        "colab_type": "text"
      },
      "source": [
        "gdrive mount & add project directory"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-cao7b1U17S3",
        "colab_type": "code",
        "outputId": "ccc6cd1a-c7b6-4ad9-9b36-eb57a34374e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\"\"\"\n",
        "Upload hyperbolic_nn folder to your Google drive root directory and run the following.\n",
        "Authentication (in-browser) is required\n",
        "\"\"\"\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/gdrive/My Drive/hyperbolic_nn')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QBGnV7o05mHh",
        "colab_type": "text"
      },
      "source": [
        "## Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih7w4zo64TYs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "import time\n",
        "import random\n",
        "from random import shuffle\n",
        "import math\n",
        "from datetime import datetime\n",
        "\n",
        "import util\n",
        "import rnn_impl"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wvCzLengbnt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "### arguments to change ###\n",
        "\"\"\"\n",
        "num_epochs: int \n",
        "word_dim: int\n",
        "cell_type: 'rnn/gru/TFrnn/TFgru/TFlstm\n",
        "geom: 'eucl/hyp'\n",
        "hyp_opt: 'rsgd/projsgd\n",
        "\"\"\"\n",
        "base_name = \"experiment_1\"\n",
        "root_path = '/content/gdrive/My Drive/hyperbolic_nn'\n",
        "num_epochs = 30\n",
        "\n",
        "is_spelling_correcred = True\n",
        "\n",
        "word_dim = 5\n",
        "hidden_dim = word_dim\n",
        "\n",
        "cell_type = 'rnn'\n",
        "geom = 'hyp'\n",
        "hyp_opt = \"rsgd\"\n",
        "\n",
        "lr_ffnn = 0.01\n",
        "lr_words = 0.1\n",
        "batch_size = 64\n",
        "\n",
        "save_model = True\n",
        "save_to_path = root_path+\"/models/\"+base_name\n",
        "\n",
        "restore_model = False\n",
        "restore_from_path = \"\"\n",
        "\n",
        "pretrained = False \n",
        "pretrained_path = root_path+\"/pretrained/{}_{}.pickle\".format(geom,word_dim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F4lHnWLB6b58",
        "colab_type": "code",
        "outputId": "9e0c7a9d-51a1-4ccf-841c-7deef5651ed4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "### No need to change below ###\n",
        "dtype = tf.float64\n",
        "\n",
        "# Project setup\n",
        "num_classes = 2\n",
        "\n",
        "if is_spelling_corrected:\n",
        "  snli_data_type = 'clean'\n",
        "else:\n",
        "  snli_data_type = 'dirty'\n",
        "\n",
        "word_to_id_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type, 'word_to_id')\n",
        "id_to_word_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type,'id_to_word')\n",
        "suffix = '_' + str(num_classes) + 'class'\n",
        "training_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'train' + suffix)\n",
        "test_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'test' + suffix)\n",
        "dev_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'dev' + suffix)\n",
        "\n",
        "PROJ_EPS = 1e-5\n",
        "util.PROJ_EPS = PROJ_EPS\n",
        "dataset = \"SNLI\"\n",
        "c_val = 1.0\n",
        "\n",
        "# FFNN params: id/relu/tanh/sigmoid\n",
        "cell_non_lin = 'id'\n",
        "ffnn_non_lin = 'id'\n",
        "\n",
        "word_init_avg_norm = 0.001\n",
        "additional_features = \"dsq\"\n",
        "\n",
        "dropout = 1.0\n",
        "\n",
        "sent_geom = geom\n",
        "inputs_geom = geom\n",
        "bias_geom = geom\n",
        "\n",
        "ffnn_geom = geom\n",
        "mlr_geom = geom\n",
        "\n",
        "before_mlr_dim = word_dim\n",
        "\n",
        "fix_biases = 'n'\n",
        "fix_biases_str = ''\n",
        "fix_matrices = 'n'\n",
        "matrices_init_eye = 'n'\n",
        "mat_str = ''\n",
        "\n",
        "# Optimization params\n",
        "burnin = 'n'\n",
        "\n",
        "# L2 regularization\n",
        "reg_beta = 0.0\n",
        "\n",
        "\n",
        "if inputs_geom == 'hyp' or bias_geom == 'hyp' or ffnn_geom =='hyp' or mlr_geom == 'hyp':\n",
        "    hyp_opt_str = hyp_opt + '_lrW' + str(lr_words) + '_lrFF' + str(lr_ffnn) + '_'\n",
        "else:\n",
        "    hyp_opt_str = ''\n",
        "\n",
        "if c_val != 1.0:\n",
        "    c_str = 'C'  + str(c_val) + '_'\n",
        "else:\n",
        "    c_str = ''\n",
        "\n",
        "if dropout != 1.0:\n",
        "    drp_str = 'drp' + str(dropout) + '_'\n",
        "else:\n",
        "    drp_str = ''\n",
        "\n",
        "burnin_str = ''\n",
        "if burnin:\n",
        "    burnin_str = 'burn' + str(burnin).lower()\n",
        "\n",
        "reg_beta_str = ''\n",
        "if reg_beta > 0.0:\n",
        "    reg_beta_str = 'reg' + str(reg_beta) + '_'\n",
        "\n",
        "additional_features_str = additional_features\n",
        "if additional_features != '':\n",
        "    additional_features_str = additional_features + '_'\n",
        "\n",
        "now = datetime.now()\n",
        "\n",
        "tensorboard_name = base_name + '_' +\\\n",
        "                   dataset + '_' +\\\n",
        "                   'W' + str(word_dim) + 'd' + str(word_init_avg_norm) + 'init_' + \\\n",
        "                   cell_type + '_' + \\\n",
        "                   'cellNonL' + cell_non_lin + '_' +\\\n",
        "                   'SENT' + sent_geom + '_' + \\\n",
        "                   'INP' + inputs_geom + '_' + \\\n",
        "                   'BIAS' + bias_geom + fix_biases_str + '_' + mat_str +\\\n",
        "                   'FFNN' + ffnn_geom + str(before_mlr_dim) + ffnn_non_lin + '_' +\\\n",
        "                   additional_features_str + \\\n",
        "                   drp_str +\\\n",
        "                   'MLR' + mlr_geom + '_' + \\\n",
        "                   reg_beta_str + \\\n",
        "                   hyp_opt_str + \\\n",
        "                   c_str +\\\n",
        "                   'prje' + str(PROJ_EPS) + '_' + \\\n",
        "                   'bs' + str(batch_size) + '_' +\\\n",
        "                   burnin_str +  '__' + now.strftime(\"%H:%M:%S%dM\")\n",
        "\n",
        "name_experiment = tensorboard_name\n",
        "logger = util.setup_logger(name_experiment, logs_dir= os.path.join(root_path, 'logs/'), also_stdout=True)\n",
        "logger.info('PARAMS :  ' + name_experiment)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2020/03/10 18:11:27: PARAMS :  experiment_1_SNLI_W5d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp5id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__18:11:2710M\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2zwJ_gTR7dsV",
        "colab_type": "code",
        "outputId": "4bab6333-ff30-430a-8b07-a0bf2e3a539e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "class HyperbolicRNNModel:\n",
        "    def __init__(self, word_to_id, id_to_word):\n",
        "        self.word_to_id = word_to_id\n",
        "        self.id_to_word = id_to_word\n",
        "\n",
        "        self.construct_placeholders()\n",
        "        self.construct_execution_graph()\n",
        "\n",
        "\n",
        "    def construct_placeholders(self):\n",
        "        self.label_placeholder = tf.placeholder(tf.int32,\n",
        "                                                shape=[batch_size],\n",
        "                                                name='label_placeholder')\n",
        "\n",
        "        self.word_ids_1 = tf.placeholder(tf.int32, shape=[batch_size, None],\n",
        "                                         name='word_ids_1_placeholder')\n",
        "        self.word_ids_2 = tf.placeholder(tf.int32, shape=[batch_size, None],\n",
        "                                         name='word_ids_2_placeholder')\n",
        "\n",
        "        self.num_words_1 = tf.placeholder(tf.int32, shape=[batch_size],\n",
        "                                          name='num_words_1_placeholder')\n",
        "        self.num_words_2 = tf.placeholder(tf.int32, shape=[batch_size],\n",
        "                                          name='num_words_2_placeholder')\n",
        "\n",
        "        self.burn_in_factor = tf.placeholder(dtype, name='burn_in_factor_placeholder')\n",
        "        self.dropout_placeholder = tf.placeholder(dtype, name='dropout_placeholder')\n",
        "\n",
        "\n",
        "    ###############################################################################################\n",
        "    def construct_execution_graph(self):\n",
        "\n",
        "        # Collect vars separately. Word embeddings are not used here.\n",
        "        eucl_vars = []\n",
        "        hyp_vars = []\n",
        "\n",
        "        ################## word embeddings ###################\n",
        "\n",
        "        # Initialize word embeddings close to 0, to have average norm equal to word_init_avg_norm.\n",
        "        maxval = (3. * (word_init_avg_norm ** 2) / (2. * word_dim)) ** (1. / 3)\n",
        "        initializer = tf.random_uniform_initializer(minval=-maxval, maxval=maxval, dtype=dtype)\n",
        "        self.embeddings = tf.get_variable('embeddings',\n",
        "                                          dtype=dtype,\n",
        "                                          shape=[len(self.word_to_id), word_dim],\n",
        "                                          initializer=initializer)\n",
        "\n",
        "        if inputs_geom == 'eucl':\n",
        "            eucl_vars += [self.embeddings]\n",
        "\n",
        "        ################## RNNs for sentence embeddings ###################\n",
        "\n",
        "        if cell_type == 'TFrnn':\n",
        "            assert sent_geom == 'eucl'\n",
        "            cell_class = lambda h_dim: tf.contrib.rnn.BasicRNNCell(h_dim)\n",
        "        elif cell_type == 'TFgru':\n",
        "            assert sent_geom == 'eucl'\n",
        "            cell_class = lambda h_dim: tf.contrib.rnn.GRUCell(h_dim)\n",
        "        elif cell_type == 'TFlstm':\n",
        "            assert sent_geom == 'eucl'\n",
        "            cell_class = lambda h_dim: tf.contrib.rnn.BasicLSTMCell(h_dim)\n",
        "        elif cell_type == 'rnn' and sent_geom == 'eucl':\n",
        "            cell_class = lambda h_dim: rnn_impl.EuclRNN(h_dim, dtype=dtype)\n",
        "        elif cell_type == 'gru' and sent_geom == 'eucl':\n",
        "            cell_class = lambda h_dim: rnn_impl.EuclGRU(h_dim, dtype=dtype)\n",
        "        elif cell_type == 'rnn' and sent_geom == 'hyp':\n",
        "            cell_class = lambda h_dim: rnn_impl.HypRNN(num_units=h_dim,\n",
        "                                                       inputs_geom=inputs_geom,\n",
        "                                                       bias_geom=bias_geom,\n",
        "                                                       c_val=c_val,\n",
        "                                                       non_lin=cell_non_lin,\n",
        "                                                       fix_biases=fix_biases,\n",
        "                                                       fix_matrices=fix_matrices,\n",
        "                                                       matrices_init_eye=matrices_init_eye,\n",
        "                                                       dtype=dtype)\n",
        "        elif cell_type == 'gru' and sent_geom == 'hyp':\n",
        "            cell_class = lambda h_dim: rnn_impl.HypGRU(num_units=h_dim,\n",
        "                                                       inputs_geom=inputs_geom,\n",
        "                                                       bias_geom=bias_geom,\n",
        "                                                       c_val=c_val,\n",
        "                                                       non_lin=cell_non_lin,\n",
        "                                                       fix_biases=fix_biases,\n",
        "                                                       fix_matrices=fix_matrices,\n",
        "                                                       matrices_init_eye=matrices_init_eye,\n",
        "                                                       dtype=dtype)\n",
        "        else:\n",
        "            logger.error('Not valid cell type: %s and sent_geom %s' % (cell_type, sent_geom))\n",
        "            exit()\n",
        "\n",
        "        # RNN 1\n",
        "        with tf.variable_scope(cell_type + '1'):\n",
        "            word_embeddings_1 = tf.nn.embedding_lookup(self.embeddings, self.word_ids_1) # bs x num_w_s1 x dim\n",
        "\n",
        "            cell_1 = cell_class(hidden_dim)\n",
        "            initial_state_1 = cell_1.zero_state(batch_size, dtype)\n",
        "            outputs_1, state_1 = tf.nn.dynamic_rnn(cell=cell_1,\n",
        "                                                   inputs=word_embeddings_1,\n",
        "                                                   dtype=dtype,\n",
        "                                                   initial_state=initial_state_1,\n",
        "                                                   sequence_length=self.num_words_1)\n",
        "            if cell_type == 'TFlstm':\n",
        "                self.sent_1 = state_1[1]\n",
        "            else:\n",
        "                self.sent_1 = state_1\n",
        "\n",
        "\n",
        "            sent1_norm = util.tf_norm(self.sent_1)\n",
        "\n",
        "\n",
        "        # RNN 2\n",
        "        with tf.variable_scope(cell_type + '2'):\n",
        "            word_embeddings_2 = tf.nn.embedding_lookup(self.embeddings, self.word_ids_2)\n",
        "            # tf.summary.scalar('word_emb2', tf.reduce_mean(tf.norm(word_embeddings_2, axis=2)))\n",
        "\n",
        "            cell_2 = cell_class(hidden_dim)\n",
        "            initial_state_2 = cell_2.zero_state(batch_size, dtype)\n",
        "            outputs_2, state_2 = tf.nn.dynamic_rnn(cell=cell_2,\n",
        "                                                   inputs=word_embeddings_2,\n",
        "                                                   dtype=dtype,\n",
        "                                                   initial_state=initial_state_2,\n",
        "                                                   sequence_length=self.num_words_2)\n",
        "            if cell_type == 'TFlstm':\n",
        "                self.sent_2 = state_2[1]\n",
        "            else:\n",
        "                self.sent_2 = state_2\n",
        "\n",
        "\n",
        "            sent2_norm = util.tf_norm(self.sent_2)\n",
        "\n",
        "\n",
        "        tf.summary.scalar('RNN/word_emb1', tf.reduce_mean(tf.norm(word_embeddings_1, axis=2)))\n",
        "        tf.summary.scalar('RNN/sent1', tf.reduce_mean(sent1_norm))\n",
        "        tf.summary.scalar('RNN/sent2', tf.reduce_mean(sent2_norm))\n",
        "\n",
        "\n",
        "        eucl_vars += cell_1.eucl_vars + cell_2.eucl_vars\n",
        "        if sent_geom == 'hyp':\n",
        "            hyp_vars += cell_1.hyp_vars + cell_2.hyp_vars\n",
        "\n",
        " \n",
        "        ## Compute d(s1, s2)\n",
        "        if sent_geom == 'eucl':\n",
        "            d_sq_s1_s2 = util.tf_euclid_dist_sq(self.sent_1, self.sent_2)\n",
        "        else:\n",
        "            d_sq_s1_s2 = util.tf_poinc_dist_sq(self.sent_1, self.sent_2, c = c_val)\n",
        "\n",
        "\n",
        "        ##### Some summaries:\n",
        "\n",
        "        # For summaries and debugging, we need these:\n",
        "        pos_labels = tf.reshape(tf.cast(self.label_placeholder, tf.float64), [-1, 1])\n",
        "        neg_labels = 1. - pos_labels\n",
        "        weights_pos_labels = pos_labels / tf.reduce_sum(pos_labels)\n",
        "        weights_neg_labels = neg_labels / tf.reduce_sum(neg_labels)\n",
        "\n",
        "        ################## first feed forward layer ###################\n",
        "\n",
        "        # Define variables for the first feed-forward layer: W1 * s1 + W2 * s2 + b + bd * d(s1,s2)\n",
        "        W_ff_s1 = tf.get_variable('W_ff_s1',\n",
        "                                  dtype=dtype,\n",
        "                                  shape=[hidden_dim, before_mlr_dim],\n",
        "                                  initializer= tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        W_ff_s2 = tf.get_variable('W_ff_s2',\n",
        "                                  dtype=dtype,\n",
        "                                  shape=[hidden_dim, before_mlr_dim],\n",
        "                                  initializer= tf.contrib.layers.xavier_initializer())\n",
        "\n",
        "        b_ff = tf.get_variable('b_ff',\n",
        "                               dtype=dtype,\n",
        "                               shape=[1, before_mlr_dim],\n",
        "                               initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        b_ff_d = tf.get_variable('b_ff_d',\n",
        "                                 dtype=dtype,\n",
        "                                 shape=[1, before_mlr_dim],\n",
        "                                 initializer=tf.constant_initializer(0.0))\n",
        "\n",
        "        eucl_vars += [W_ff_s1, W_ff_s2]\n",
        "        if ffnn_geom == 'eucl' or bias_geom == 'eucl':\n",
        "            eucl_vars += [b_ff]\n",
        "            if additional_features == 'dsq':\n",
        "                eucl_vars += [b_ff_d]\n",
        "        else:\n",
        "            hyp_vars += [b_ff]\n",
        "            if additional_features == 'dsq':\n",
        "                hyp_vars += [b_ff_d]\n",
        "\n",
        "\n",
        "        if ffnn_geom == 'eucl' and sent_geom == 'hyp': # Sentence embeddings are Euclidean after log, except the proper distance (Eucl or hyp) is kept!\n",
        "            self.sent_1 = util.tf_log_map_zero(self.sent_1, c_val)\n",
        "            self.sent_2 = util.tf_log_map_zero(self.sent_2, c_val)\n",
        "\n",
        "        ####### Build output_ffnn #######\n",
        "        if ffnn_geom == 'eucl':\n",
        "            output_ffnn = tf.matmul(self.sent_1, W_ff_s1) + tf.matmul(self.sent_2, W_ff_s2) + b_ff\n",
        "            if additional_features == 'dsq': # [u, v, d(u,v)^2]\n",
        "                output_ffnn = output_ffnn + d_sq_s1_s2 * b_ff_d\n",
        "\n",
        "        else:\n",
        "            assert sent_geom == 'hyp'\n",
        "            ffnn_s1 = util.tf_mob_mat_mul(W_ff_s1, self.sent_1, c_val)\n",
        "            ffnn_s2 = util.tf_mob_mat_mul(W_ff_s2, self.sent_2, c_val)\n",
        "            output_ffnn = util.tf_mob_add(ffnn_s1, ffnn_s2, c_val)\n",
        "\n",
        "            hyp_b_ff = b_ff\n",
        "            if bias_geom == 'eucl':\n",
        "                hyp_b_ff = util.tf_exp_map_zero(b_ff, c_val)\n",
        "            output_ffnn = util.tf_mob_add(output_ffnn, hyp_b_ff, c_val)\n",
        "\n",
        "            if additional_features == 'dsq': # [u, v, d(u,v)^2]\n",
        "                hyp_b_ff_d = b_ff_d\n",
        "                if bias_geom == 'eucl':\n",
        "                    hyp_b_ff_d = util.tf_exp_map_zero(b_ff_d, c_val)\n",
        "\n",
        "                output_ffnn = util.tf_mob_add(output_ffnn,\n",
        "                                              util.tf_mob_scalar_mul(d_sq_s1_s2, hyp_b_ff_d, c_val),\n",
        "                                              c_val)\n",
        "\n",
        "        if ffnn_geom == 'eucl':\n",
        "            output_ffnn = util.tf_eucl_non_lin(output_ffnn, non_lin=ffnn_non_lin)\n",
        "        else:\n",
        "            output_ffnn = util.tf_hyp_non_lin(output_ffnn,\n",
        "                                              non_lin=ffnn_non_lin,\n",
        "                                              hyp_output = (mlr_geom == 'hyp' and dropout == 1.0),\n",
        "                                              c=c_val)\n",
        "        # Mobius dropout\n",
        "        if dropout < 1.0:\n",
        "            # If we are here, then output_ffnn should be Euclidean.\n",
        "            output_ffnn = tf.nn.dropout(output_ffnn, keep_prob=self.dropout_placeholder)\n",
        "            if (mlr_geom == 'hyp'):\n",
        "                output_ffnn = util.tf_exp_map_zero(output_ffnn, c_val)\n",
        "\n",
        "\n",
        "        ################## MLR ###################\n",
        "        # output_ffnn is batch_size x before_mlr_dim\n",
        "\n",
        "        A_mlr = []\n",
        "        P_mlr = []\n",
        "        logits_list = []\n",
        "        for cl in range(num_classes):\n",
        "            A_mlr.append(tf.get_variable('A_mlr' + str(cl),\n",
        "                                         dtype=dtype,\n",
        "                                         shape=[1, before_mlr_dim],\n",
        "                                         initializer=tf.contrib.layers.xavier_initializer()))\n",
        "            eucl_vars += [A_mlr[cl]]\n",
        "\n",
        "            P_mlr.append(tf.get_variable('P_mlr' + str(cl),\n",
        "                                         dtype=dtype,\n",
        "                                         shape=[1, before_mlr_dim],\n",
        "                                         initializer=tf.constant_initializer(0.0)))\n",
        "\n",
        "            if mlr_geom == 'eucl':\n",
        "                eucl_vars += [P_mlr[cl]]\n",
        "                logits_list.append(tf.reshape(util.tf_dot(-P_mlr[cl] + output_ffnn, A_mlr[cl]), [-1]))\n",
        "\n",
        "            elif mlr_geom == 'hyp':\n",
        "                hyp_vars += [P_mlr[cl]]\n",
        "                minus_p_plus_x = util.tf_mob_add(-P_mlr[cl], output_ffnn, c_val)\n",
        "                norm_a = util.tf_norm(A_mlr[cl])\n",
        "                lambda_px = util.tf_lambda_x(minus_p_plus_x, c_val)\n",
        "                px_dot_a = util.tf_dot(minus_p_plus_x, tf.nn.l2_normalize(A_mlr[cl]))\n",
        "                logit = 2. / np.sqrt(c_val) * norm_a * tf.asinh(np.sqrt(c_val) * px_dot_a * lambda_px)\n",
        "                logits_list.append(tf.reshape(logit, [-1]))\n",
        "\n",
        "        self.logits = tf.stack(logits_list, axis=1)\n",
        "\n",
        "        self.argmax_idx = tf.argmax(self.logits, axis=1, output_type=tf.int32)\n",
        "\n",
        "        self.loss = tf.reduce_mean(\n",
        "            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_placeholder,\n",
        "                                                           logits=self.logits))\n",
        "        tf.summary.scalar('classif/unreg_loss', self.loss)\n",
        "\n",
        "        if reg_beta > 0.0:\n",
        "            assert num_classes == 2\n",
        "            distance_regularizer = tf.reduce_mean(\n",
        "                (tf.cast(self.label_placeholder, dtype=dtype) - 0.5) * d_sq_s1_s2)\n",
        "\n",
        "            self.loss = self.loss + reg_beta * distance_regularizer\n",
        "\n",
        "        self.acc = tf.reduce_mean(tf.to_float(tf.equal(self.argmax_idx, self.label_placeholder)))\n",
        "        tf.summary.scalar('classif/accuracy', self.acc)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        ######################################## OPTIMIZATION ######################################\n",
        "        all_updates_ops = []\n",
        "\n",
        "        ###### Update Euclidean parameters using Adam.\n",
        "        optimizer_euclidean_params = tf.train.AdamOptimizer(learning_rate=1e-3)\n",
        "        eucl_grads = optimizer_euclidean_params.compute_gradients(self.loss, eucl_vars)\n",
        "        capped_eucl_gvs = [(tf.clip_by_norm(grad, 1.), var) for grad, var in eucl_grads]  ###### Clip gradients\n",
        "        all_updates_ops.append(optimizer_euclidean_params.apply_gradients(capped_eucl_gvs))\n",
        "\n",
        "\n",
        "        ###### Update Hyperbolic parameters, i.e. word embeddings and some biases in our case.\n",
        "        def rsgd(v, riemannian_g, learning_rate):\n",
        "            if hyp_opt == 'rsgd':\n",
        "                return util.tf_exp_map_x(v, -self.burn_in_factor * learning_rate * riemannian_g, c=c_val)\n",
        "            else:\n",
        "                # Use approximate RSGD based on a simple retraction.\n",
        "                updated_v = v - self.burn_in_factor * learning_rate * riemannian_g\n",
        "                # Projection op after SGD update. Need to make sure embeddings are inside the unit ball.\n",
        "                return util.tf_project_hyp_vecs(updated_v, c_val)\n",
        "\n",
        "\n",
        "        if inputs_geom == 'hyp':\n",
        "            grads_and_indices_hyp_words = tf.gradients(self.loss, self.embeddings)\n",
        "            grads_hyp_words = grads_and_indices_hyp_words[0].values\n",
        "            repeating_indices = grads_and_indices_hyp_words[0].indices\n",
        "            unique_indices, idx_in_repeating_indices = tf.unique(repeating_indices)\n",
        "            agg_gradients = tf.unsorted_segment_sum(grads_hyp_words,\n",
        "                                                    idx_in_repeating_indices,\n",
        "                                                    tf.shape(unique_indices)[0])\n",
        "\n",
        "            agg_gradients = tf.clip_by_norm(agg_gradients, 1.) ######## Clip gradients\n",
        "            unique_word_emb = tf.nn.embedding_lookup(self.embeddings, unique_indices)  # no repetitions here\n",
        "\n",
        "            riemannian_rescaling_factor = util.riemannian_gradient_c(unique_word_emb, c=c_val)\n",
        "            rescaled_gradient = riemannian_rescaling_factor * agg_gradients\n",
        "\n",
        "            all_updates_ops.append(tf.scatter_update(self.embeddings,\n",
        "                                                     unique_indices,\n",
        "                                                     rsgd(unique_word_emb, rescaled_gradient, lr_words))) # Updated rarely\n",
        "\n",
        "        if len(hyp_vars) > 0:\n",
        "            hyp_grads = tf.gradients(self.loss, hyp_vars)\n",
        "            capped_hyp_grads = [tf.clip_by_norm(grad, 1.) for grad in hyp_grads]  ###### Clip gradients\n",
        "\n",
        "\n",
        "            for i in range(len(hyp_vars)):\n",
        "                riemannian_rescaling_factor = util.riemannian_gradient_c(hyp_vars[i], c=c_val)\n",
        "                rescaled_gradient = riemannian_rescaling_factor * capped_hyp_grads[i]\n",
        "                all_updates_ops.append(tf.assign(hyp_vars[i], rsgd(hyp_vars[i], rescaled_gradient, lr_ffnn)))  # Updated frequently\n",
        "\n",
        "        self.all_optimizer_var_updates_op = tf.group(*all_updates_ops)\n",
        "\n",
        "\n",
        "        self.summary_merged = tf.summary.merge_all()\n",
        "        self.test_summary_writer = tf.summary.FileWriter(\n",
        "            os.path.join(root_path, 'tb_28may/' + tensorboard_name + '/'))\n",
        "\n",
        "\n",
        "    ###############################################################################################\n",
        "    ###############################################################################################\n",
        "    ###############################################################################################\n",
        "    def test(self, sess, test_or_valid_data, name, summary_i):\n",
        "        N = len(test_or_valid_data)\n",
        "        i = 0\n",
        "        predictions = []\n",
        "\n",
        "        avg_loss = 0.0\n",
        "        num_b_in_loss = 0\n",
        "        while i < N:\n",
        "            batch_word_ids_1, batch_num_words_1, batch_word_ids_2, batch_num_words_2, batch_label = self.next_batch(\n",
        "                i=i, N=N, data=test_or_valid_data)\n",
        "\n",
        "            if name == 'test':\n",
        "                summary, loss, argmax_idx = \\\n",
        "                    sess.run([self.summary_merged, self.loss, self.argmax_idx], feed_dict={\n",
        "                        self.word_ids_1: batch_word_ids_1,\n",
        "                        self.num_words_1: batch_num_words_1,\n",
        "                        self.word_ids_2: batch_word_ids_2,\n",
        "                        self.num_words_2: batch_num_words_2,\n",
        "                        self.label_placeholder: batch_label,\n",
        "                        self.dropout_placeholder: 1.0\n",
        "                    })\n",
        "                self.test_summary_writer.add_summary(summary, summary_i)\n",
        "\n",
        "            else:\n",
        "                loss, argmax_idx = \\\n",
        "                    sess.run([self.loss, self.argmax_idx], feed_dict={\n",
        "                        self.word_ids_1: batch_word_ids_1,\n",
        "                        self.num_words_1: batch_num_words_1,\n",
        "                        self.word_ids_2: batch_word_ids_2,\n",
        "                        self.num_words_2: batch_num_words_2,\n",
        "                        self.label_placeholder: batch_label,\n",
        "                        self.dropout_placeholder: 1.0\n",
        "                    })\n",
        "\n",
        "            avg_loss += loss\n",
        "            num_b_in_loss += 1\n",
        "\n",
        "            for ci in argmax_idx:\n",
        "                predictions.append(ci)\n",
        "\n",
        "            i += batch_size\n",
        "\n",
        "        avg_loss /= num_b_in_loss\n",
        "\n",
        "        num_correct = 0.0\n",
        "        glE_predE = 0.0\n",
        "        glN_predN = 0.0\n",
        "        glE_predN = 0.0\n",
        "        glN_predE = 0.0\n",
        "\n",
        "        predictions = predictions[:N]\n",
        "\n",
        "        for ind, predicted_label in enumerate(predictions):\n",
        "            gold_label = test_or_valid_data[ind][4]\n",
        "\n",
        "            if predicted_label == gold_label:\n",
        "                num_correct += 1.0\n",
        "                if predicted_label == 1:\n",
        "                    glE_predE += 1.0\n",
        "                else:\n",
        "                    glN_predN += 1.0\n",
        "            else:\n",
        "                if predicted_label == 1:\n",
        "                    glN_predE += 1.0\n",
        "                else:\n",
        "                    glE_predN += 1.0\n",
        "\n",
        "        accuracy = num_correct / (1.0 * N)\n",
        "\n",
        "        if name == 'test':\n",
        "            logger.info('For ' + name + ': ==> ' +\n",
        "                        ' glN_predN = ' + str(glN_predN) + '; glE_predN = ' + str(glE_predN) +\n",
        "                        '; glN_predE = ' + str(glN_predE) + '; glE_predE = ' + str(glE_predE))\n",
        "\n",
        "        return accuracy\n",
        "\n",
        "\n",
        "    def next_batch(self, i, N, data):\n",
        "        it = i\n",
        "        to = min(i + batch_size, N)\n",
        "\n",
        "        MAX_PAD = 0\n",
        "        batch_word_ids_1 = []\n",
        "        batch_num_words_1 = []\n",
        "\n",
        "        batch_word_ids_2 = []\n",
        "        batch_num_words_2 = []\n",
        "\n",
        "        batch_label = []\n",
        "\n",
        "        while it < to:\n",
        "            word_ids_1 = data[it][0]\n",
        "            num_words_1 = data[it][1]\n",
        "            word_ids_2 = data[it][2]\n",
        "            num_words_2 = data[it][3]\n",
        "            label = data[it][4]\n",
        "\n",
        "            MAX_PAD = max(MAX_PAD, max(len(word_ids_1), len(word_ids_2)))\n",
        "\n",
        "            batch_word_ids_1.append(word_ids_1)\n",
        "            batch_num_words_1.append(num_words_1)\n",
        "\n",
        "            batch_word_ids_2.append(word_ids_2)\n",
        "            batch_num_words_2.append(num_words_2)\n",
        "\n",
        "            batch_label.append(label)\n",
        "\n",
        "            it += 1\n",
        "\n",
        "            if it == to and to == N:\n",
        "                it = 0\n",
        "                to = batch_size - len(batch_word_ids_1)\n",
        "\n",
        "        for ind in range(0, batch_size):\n",
        "\n",
        "            while len(batch_word_ids_1[ind]) < MAX_PAD:\n",
        "                batch_word_ids_1[ind].append(0)\n",
        "\n",
        "            while len(batch_word_ids_2[ind]) < MAX_PAD:\n",
        "                batch_word_ids_2[ind].append(0)\n",
        "\n",
        "            if len(batch_word_ids_1[ind]) != MAX_PAD or len(batch_word_ids_2[ind]) != MAX_PAD:\n",
        "                logger.error('THIS IS WRONG!: len1: %d\\nlen2: %d\\nMAX_PAD:%d\\n' %\n",
        "                             (len(batch_word_ids_1[ind]), len(batch_word_ids_2[ind]), MAX_PAD))\n",
        "                exit()\n",
        "\n",
        "        batch_word_ids_1 = np.array(batch_word_ids_1)\n",
        "        batch_num_words_1 = np.array(batch_num_words_1)\n",
        "\n",
        "        batch_word_ids_2 = np.array(batch_word_ids_2)\n",
        "        batch_num_words_2 = np.array(batch_num_words_2)\n",
        "        batch_label = np.array(batch_label)\n",
        "\n",
        "        return batch_word_ids_1, batch_num_words_1, batch_word_ids_2, batch_num_words_2, batch_label\n",
        "\n",
        "\n",
        "    def dataset_to_minibatches(self, dataset):\n",
        "        N = len(dataset)\n",
        "        i = 0\n",
        "        batches_list = []\n",
        "        while i < N:\n",
        "            batches_list.append(self.next_batch(i=i, N=N, data=dataset))\n",
        "            i += batch_size\n",
        "        return batches_list\n",
        "\n",
        "\n",
        "    def train(self, training_data, dev_data, test_data, restore_model=False, save_model=False,\n",
        "              restore_from_path=None, save_to_path=None):\n",
        "\n",
        "        training_data_batches = self.dataset_to_minibatches(training_data)\n",
        "        num_batches = len(training_data_batches)\n",
        "        N = len(training_data)\n",
        "\n",
        "        saver = tf.train.Saver()\n",
        "        best_test_accuracy = 0.0\n",
        "        best_validation_accuracy = 0.0\n",
        "        best_i = 0\n",
        "\n",
        "        PRINT_STEP = 2000\n",
        "\n",
        "        config = tf.ConfigProto(\n",
        "            intra_op_parallelism_threads=5,\n",
        "            inter_op_parallelism_threads=3,\n",
        "            log_device_placement=True\n",
        "        )\n",
        "        config.gpu_options.allow_growth = True\n",
        "        with tf.Session(config=config) as sess:\n",
        "\n",
        "            if restore_model:\n",
        "                saver.restore(sess, restore_from_path)\n",
        "            else:\n",
        "                sess.run(tf.global_variables_initializer())\n",
        "\n",
        "            sess.graph.finalize()\n",
        "\n",
        "            burn_in_factor = 1.0\n",
        "            epoch = -1\n",
        "            while epoch < num_epochs:\n",
        "                epoch += 1\n",
        "                logger.info('Epoch: %d' % epoch)\n",
        "\n",
        "                cur_total_time = 0\n",
        "                shuffle(training_data_batches) # Shuffle training data after each epoch.\n",
        "\n",
        "                if burnin and epoch == 0:\n",
        "                    burn_in_factor /= 10.0\n",
        "                i = -1\n",
        "                while i < num_batches - 1:\n",
        "                    i += 1\n",
        "\n",
        "                    # Training\n",
        "                    batch_word_ids_1, batch_num_words_1, \\\n",
        "                    batch_word_ids_2, batch_num_words_2, \\\n",
        "                    batch_label = training_data_batches[i]\n",
        "\n",
        "                    feed_dict = {\n",
        "                        self.word_ids_1: batch_word_ids_1,\n",
        "                        self.num_words_1: batch_num_words_1,\n",
        "                        self.word_ids_2: batch_word_ids_2,\n",
        "                        self.num_words_2: batch_num_words_2,\n",
        "                        self.label_placeholder: batch_label,\n",
        "                        self.burn_in_factor: burn_in_factor,\n",
        "                        self.dropout_placeholder: dropout\n",
        "                    }\n",
        "\n",
        "                    sess_time_start = time.time()\n",
        "                    curr_loss, _ = sess.run([self.loss, self.all_optimizer_var_updates_op], feed_dict=feed_dict)\n",
        "                    cur_total_time += time.time() - sess_time_start\n",
        "\n",
        "                    if i % PRINT_STEP == 0:\n",
        "                        if i > 0:\n",
        "                            avg_sec_per_sent = cur_total_time / (PRINT_STEP * batch_size)\n",
        "                            logger.info('Num examples processed: %d. curr_loss: %.4f; sec_per_sent: %.4f' % (\n",
        "                                epoch * N + i * batch_size, curr_loss, avg_sec_per_sent))\n",
        "                        cur_total_time = 0\n",
        "\n",
        "\n",
        "                        # Testing\n",
        "                        validation_accuracy = self.test(sess,\n",
        "                                                        dev_data,\n",
        "                                                        'validation',\n",
        "                                                        epoch * N + i * batch_size)\n",
        "                        test_accuracy = self.test(sess,\n",
        "                                                  test_data,\n",
        "                                                  'test',\n",
        "                                                  epoch * N + i * batch_size)\n",
        "                        logger.info('CURRENT val accuracy: %.4f ; test accuracy: \\033[92m %.4f \\033[0m' %\n",
        "                                    (validation_accuracy, test_accuracy))\n",
        "\n",
        "                        if validation_accuracy > best_validation_accuracy:\n",
        "                            best_validation_accuracy = validation_accuracy\n",
        "                            best_test_accuracy = test_accuracy\n",
        "                            best_i = epoch * N + i * batch_size\n",
        "\n",
        "                        logger.info(('BEST: i = %d, val acc: ' + '\\033[94m' + ' %.2f' + '\\033[0m' +\n",
        "                                     ', test acc: ' + '\\033[91m' + ' %.2f' + '\\033[0m') %\n",
        "                                    (best_i, 100 * best_validation_accuracy, 100 * best_test_accuracy))\n",
        "\n",
        "                        if save_model:\n",
        "                            store_time_begin = time.time()\n",
        "                            saver.save(sess, '%s_epoch_%d_it_%d.ckpt' % (save_to_path, epoch, i))\n",
        "                            logger.info('Stored the model in %d seconds.' %\n",
        "                                        (time.time() - store_time_begin))\n",
        "\n",
        "                        logger.info('EXPERIMENT = ' + name_experiment)\n",
        "                        logger.info('=============================================================')\n",
        "\n",
        "                    if np.isinf(curr_loss) or np.isnan(curr_loss):\n",
        "                        logger.error('At example ' + str(epoch * N + i * batch_size) +\n",
        "                                     '; curr_loss: ' + str(curr_loss))\n",
        "                        exit()\n",
        "\n",
        "\n",
        "        logger.info(('DONE -- BEST: i = %d, val acc: ' + '\\033[94m' + ' %.2f' + '\\033[0m' +\n",
        "                     ', test acc: ' + '\\033[91m' + ' %.2f' + '\\033[0m') %\n",
        "                    (best_i, 100 * best_validation_accuracy, 100 * best_test_accuracy))\n",
        "\n",
        "\n",
        "def get_datasets():\n",
        "    logger.info('Loading train - val - test data')\n",
        "\n",
        "    test_data = pickle.load(open(test_data_file_path, 'rb'))\n",
        "    dev_data = pickle.load(open(dev_data_file_path, 'rb'))\n",
        "    training_data = pickle.load(open(training_data_file_path, 'rb'))\n",
        "\n",
        "    logger.info('Training data size: %d' % len(training_data))\n",
        "    class_to_count = {1: 0.0, 0: 0.0}\n",
        "    for i in range(len(training_data)):\n",
        "        class_to_count[training_data[i][4]] += 1.0\n",
        "    for cl in class_to_count:\n",
        "        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(training_data)))\n",
        "\n",
        "    logger.info('Validation data size: %d' % len(dev_data))\n",
        "    class_to_count = {1: 0.0, 0: 0.0}\n",
        "    for i in range(len(test_data)):\n",
        "        class_to_count[test_data[i][4]] += 1.0\n",
        "    for cl in class_to_count:\n",
        "        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(dev_data)))\n",
        "\n",
        "    logger.info('Test data size: %d' % len(test_data))\n",
        "    class_to_count = {1: 0.0, 0: 0.0}\n",
        "    for i in range(len(test_data)):\n",
        "        class_to_count[test_data[i][4]] += 1.0\n",
        "    for cl in class_to_count:\n",
        "        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(test_data)))\n",
        "\n",
        "    return training_data, dev_data, test_data\n",
        "\n",
        "\n",
        "def run():\n",
        "    word_to_id = pickle.load(open(word_to_id_file_path, 'rb'))\n",
        "    id_to_word = pickle.load(open(id_to_word_file_path, 'rb'))\n",
        "\n",
        "    model = HyperbolicRNNModel(word_to_id=word_to_id,\n",
        "                               id_to_word=id_to_word)\n",
        "\n",
        "    training_data, dev_data, test_data = get_datasets()\n",
        "\n",
        "    model.train(training_data=training_data,\n",
        "                dev_data=dev_data,\n",
        "                test_data=test_data,\n",
        "                save_model=save_model,\n",
        "                save_to_path=save_to_path,\n",
        "                restore_model=restore_model,\n",
        "                restore_from_path=restore_from_path)\n",
        "\n",
        "if __name__=='__main__':\n",
        "  run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-11-14bd699c30fb>:40: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-11-14bd699c30fb>:98: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "Init RNN cell\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Init RNN cell\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-11-14bd699c30fb>:280: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020/03/10 18:11:52: Loading train - val - test data\n",
            "2020/03/10 18:11:54: Training data size: 549367\n",
            "2020/03/10 18:11:54: Class 1 has 33.3868 percent samples\n",
            "2020/03/10 18:11:54: Class 0 has 66.6132 percent samples\n",
            "2020/03/10 18:11:54: Validation data size: 9842\n",
            "2020/03/10 18:11:54: Class 1 has 34.2207 percent samples\n",
            "2020/03/10 18:11:54: Class 0 has 65.5964 percent samples\n",
            "2020/03/10 18:11:54: Test data size: 9824\n",
            "2020/03/10 18:11:54: Class 1 has 34.2834 percent samples\n",
            "2020/03/10 18:11:54: Class 0 has 65.7166 percent samples\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Device mapping:\n",
            "/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n",
            "/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n",
            "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020/03/10 18:12:01: Epoch: 0\n",
            "2020/03/10 18:12:25: For test: ==>  glN_predN = 3293.0; glE_predN = 1894.0; glN_predE = 3163.0; glE_predE = 1474.0\n",
            "2020/03/10 18:12:25: CURRENT val accuracy: 0.4886 ; test accuracy: \u001b[92m 0.4852 \u001b[0m\n",
            "2020/03/10 18:12:25: BEST: i = 0, val acc: \u001b[94m 48.86\u001b[0m, test acc: \u001b[91m 48.52\u001b[0m\n",
            "2020/03/10 18:12:26: Stored the model in 0 seconds.\n",
            "2020/03/10 18:12:26: EXPERIMENT = experiment_1_SNLI_W5d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp5id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__18:11:2710M\n",
            "2020/03/10 18:12:26: =============================================================\n",
            "2020/03/10 18:18:52: Num examples processed: 128000. curr_loss: 0.4979; sec_per_sent: 0.0030\n",
            "2020/03/10 18:19:14: For test: ==>  glN_predN = 5677.0; glE_predN = 1921.0; glN_predE = 779.0; glE_predE = 1447.0\n",
            "2020/03/10 18:19:14: CURRENT val accuracy: 0.7293 ; test accuracy: \u001b[92m 0.7252 \u001b[0m\n",
            "2020/03/10 18:19:14: BEST: i = 128000, val acc: \u001b[94m 72.93\u001b[0m, test acc: \u001b[91m 72.52\u001b[0m\n",
            "2020/03/10 18:19:15: Stored the model in 0 seconds.\n",
            "2020/03/10 18:19:15: EXPERIMENT = experiment_1_SNLI_W5d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp5id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__18:11:2710M\n",
            "2020/03/10 18:19:15: =============================================================\n",
            "2020/03/10 18:25:30: Num examples processed: 256000. curr_loss: 0.5561; sec_per_sent: 0.0029\n",
            "2020/03/10 18:25:51: For test: ==>  glN_predN = 5572.0; glE_predN = 1592.0; glN_predE = 884.0; glE_predE = 1776.0\n",
            "2020/03/10 18:25:51: CURRENT val accuracy: 0.7406 ; test accuracy: \u001b[92m 0.7480 \u001b[0m\n",
            "2020/03/10 18:25:51: BEST: i = 256000, val acc: \u001b[94m 74.06\u001b[0m, test acc: \u001b[91m 74.80\u001b[0m\n",
            "2020/03/10 18:25:51: Stored the model in 0 seconds.\n",
            "2020/03/10 18:25:51: EXPERIMENT = experiment_1_SNLI_W5d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp5id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__18:11:2710M\n",
            "2020/03/10 18:25:51: =============================================================\n",
            "2020/03/10 18:32:14: Num examples processed: 384000. curr_loss: 0.3829; sec_per_sent: 0.0030\n",
            "2020/03/10 18:32:35: For test: ==>  glN_predN = 5667.0; glE_predN = 1738.0; glN_predE = 789.0; glE_predE = 1630.0\n",
            "2020/03/10 18:32:35: CURRENT val accuracy: 0.7483 ; test accuracy: \u001b[92m 0.7428 \u001b[0m\n",
            "2020/03/10 18:32:35: BEST: i = 384000, val acc: \u001b[94m 74.83\u001b[0m, test acc: \u001b[91m 74.28\u001b[0m\n",
            "2020/03/10 18:32:36: Stored the model in 0 seconds.\n",
            "2020/03/10 18:32:36: EXPERIMENT = experiment_1_SNLI_W5d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp5id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__18:11:2710M\n",
            "2020/03/10 18:32:36: =============================================================\n",
            "2020/03/10 18:38:57: Num examples processed: 512000. curr_loss: 0.4726; sec_per_sent: 0.0030\n",
            "2020/03/10 18:39:18: For test: ==>  glN_predN = 5719.0; glE_predN = 1738.0; glN_predE = 737.0; glE_predE = 1630.0\n",
            "2020/03/10 18:39:18: CURRENT val accuracy: 0.7538 ; test accuracy: \u001b[92m 0.7481 \u001b[0m\n",
            "2020/03/10 18:39:18: BEST: i = 512000, val acc: \u001b[94m 75.38\u001b[0m, test acc: \u001b[91m 74.81\u001b[0m\n",
            "2020/03/10 18:39:18: Stored the model in 0 seconds.\n",
            "2020/03/10 18:39:18: EXPERIMENT = experiment_1_SNLI_W5d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp5id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__18:11:2710M\n",
            "2020/03/10 18:39:18: =============================================================\n",
            "2020/03/10 18:41:11: Epoch: 1\n",
            "2020/03/10 18:41:33: For test: ==>  glN_predN = 5495.0; glE_predN = 1469.0; glN_predE = 961.0; glE_predE = 1899.0\n",
            "2020/03/10 18:41:33: CURRENT val accuracy: 0.7524 ; test accuracy: \u001b[92m 0.7526 \u001b[0m\n",
            "2020/03/10 18:41:33: BEST: i = 512000, val acc: \u001b[94m 75.38\u001b[0m, test acc: \u001b[91m 74.81\u001b[0m\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2020/03/10 18:41:34: Stored the model in 0 seconds.\n",
            "2020/03/10 18:41:34: EXPERIMENT = experiment_1_SNLI_W5d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp5id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__18:11:2710M\n",
            "2020/03/10 18:41:34: =============================================================\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-14bd699c30fb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 650\u001b[0;31m   \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-14bd699c30fb>\u001b[0m in \u001b[0;36mrun\u001b[0;34m()\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0msave_to_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_to_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m                 \u001b[0mrestore_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrestore_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m                 restore_from_path=restore_from_path)\n\u001b[0m\u001b[1;32m    648\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-14bd699c30fb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_data, dev_data, test_data, restore_model, save_model, restore_from_path, save_to_path)\u001b[0m\n\u001b[1;32m    547\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m                     \u001b[0msess_time_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m                     \u001b[0mcurr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_optimizer_var_updates_op\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m                     \u001b[0mcur_total_time\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msess_time_start\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}