{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"run_experiment.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qLMTdd5m49Eq","colab_type":"text"},"source":["### Environment Setup"]},{"cell_type":"markdown","metadata":{"id":"V-0rHFza5Nkl","colab_type":"text"},"source":["##### GPU config"]},{"cell_type":"code","metadata":{"id":"_d-kNkHa4-px","colab_type":"code","colab":{}},"source":["\"\"\"\n","Runtime -> Change runtime type -> Hardware Accelorator (change from None to GPU)\n","Make sure there is no assertion error below\n","\"\"\"\n","\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","assert tf.test.gpu_device_name() != '', \"NO GPU DETECTED\""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GLUoFdTf5QKM","colab_type":"text"},"source":["gdrive mount & add project directory"]},{"cell_type":"code","metadata":{"id":"-cao7b1U17S3","colab_type":"code","outputId":"73db0ef7-be5a-4a23-cbe8-6d5f47040ff1","executionInfo":{"status":"ok","timestamp":1584202149792,"user_tz":0,"elapsed":21619,"user":{"displayName":"Michael McRandom","photoUrl":"","userId":"06400918288904169628"}},"colab":{"base_uri":"https://localhost:8080/","height":124}},"source":["\"\"\"\n","Upload hyperbolic_nn folder to your Google drive root directory and run the following.\n","Authentication (in-browser) is required\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import sys\n","sys.path.append('/content/gdrive/My Drive/hyperbolic_nn')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QBGnV7o05mHh","colab_type":"text"},"source":["## Experiment"]},{"cell_type":"code","metadata":{"id":"ih7w4zo64TYs","colab_type":"code","outputId":"8e4cb554-30a9-404d-900f-12abd086a576","executionInfo":{"status":"ok","timestamp":1584202155590,"user_tz":0,"elapsed":2290,"user":{"displayName":"Michael McRandom","photoUrl":"","userId":"06400918288904169628"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"source":["import os\n","import numpy as np\n","import pickle\n","import time\n","import random\n","from random import shuffle\n","import math\n","from datetime import datetime\n","\n","import util\n","import rnn_impl"],"execution_count":3,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/gdrive/My Drive/hyperbolic_nn/rnn_impl.py:4: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0wvCzLengbnt","colab_type":"code","colab":{}},"source":["### arguments to change ###\n","\"\"\"\n","num_epochs: int \n","word_dim: int\n","cell_type: 'rnn/gru/TFrnn/TFgru/TFlstm\n","geom: 'eucl/hyp'\n","hyp_opt: 'rsgd/projsgd\n","\"\"\"\n","base_name = \"\"\n","root_path = '/content/gdrive/My Drive/hyperbolic_nn'\n","num_epochs = 30\n","\n","is_spelling_corrected = True\n","\n","word_dim = 100\n","hidden_dim = word_dim\n","\n","cell_type = 'rnn'\n","geom = 'hyp'\n","hyp_opt = \"rsgd\"\n","\n","lr_ffnn = 0.01\n","lr_words = 0.1\n","batch_size = 64\n","\n","save_model = True\n","save_to_path = root_path+\"/models/\"+base_name\n","\n","restore_model = False\n","restore_from_path = root_path + \"/models/mofo_100d_epoch_6_it_0.ckpt\"\n","\n","# Add the name of the embeddings file\n","pretrained = True\n","pretrained_path = root_path + \"/100d_embedds/100d_snli_embedds_glove\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4lHnWLB6b58","colab_type":"code","outputId":"b747f790-e3c8-496e-c87b-aa29b51bf032","executionInfo":{"status":"ok","timestamp":1584202245578,"user_tz":0,"elapsed":1013,"user":{"displayName":"Michael McRandom","photoUrl":"","userId":"06400918288904169628"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["### No need to change below ###\n","dtype = tf.float64\n","\n","# Project setup\n","num_classes = 2\n","\n","if is_spelling_corrected:\n","  snli_data_type = 'clean'\n","else:\n","  snli_data_type = 'dirty'\n","\n","word_to_id_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type, 'word_to_id')\n","id_to_word_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type,'id_to_word')\n","suffix = '_' + str(num_classes) + 'class'\n","training_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'train' + suffix)\n","test_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'test' + suffix)\n","dev_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'dev' + suffix)\n","\n","PROJ_EPS = 1e-5\n","util.PROJ_EPS = PROJ_EPS\n","dataset = \"SNLI\"\n","c_val = 1.0\n","\n","# FFNN params: id/relu/tanh/sigmoid\n","cell_non_lin = 'id'\n","ffnn_non_lin = 'id'\n","\n","word_init_avg_norm = 0.001\n","additional_features = \"dsq\"\n","\n","dropout = 1.0\n","\n","sent_geom = geom\n","inputs_geom = geom\n","bias_geom = geom\n","\n","ffnn_geom = geom\n","mlr_geom = geom\n","\n","before_mlr_dim = word_dim\n","\n","fix_biases = 'n'\n","fix_biases_str = ''\n","fix_matrices = 'n'\n","matrices_init_eye = 'n'\n","mat_str = ''\n","\n","# Optimization params\n","burnin = 'n'\n","\n","# L2 regularization\n","reg_beta = 0.0\n","\n","\n","if inputs_geom == 'hyp' or bias_geom == 'hyp' or ffnn_geom =='hyp' or mlr_geom == 'hyp':\n","    hyp_opt_str = hyp_opt + '_lrW' + str(lr_words) + '_lrFF' + str(lr_ffnn) + '_'\n","else:\n","    hyp_opt_str = ''\n","\n","if c_val != 1.0:\n","    c_str = 'C'  + str(c_val) + '_'\n","else:\n","    c_str = ''\n","\n","if dropout != 1.0:\n","    drp_str = 'drp' + str(dropout) + '_'\n","else:\n","    drp_str = ''\n","\n","burnin_str = ''\n","if burnin:\n","    burnin_str = 'burn' + str(burnin).lower()\n","\n","reg_beta_str = ''\n","if reg_beta > 0.0:\n","    reg_beta_str = 'reg' + str(reg_beta) + '_'\n","\n","additional_features_str = additional_features\n","if additional_features != '':\n","    additional_features_str = additional_features + '_'\n","\n","now = datetime.now()\n","\n","tensorboard_name = base_name + '_' +\\\n","                   dataset + '_' +\\\n","                   'W' + str(word_dim) + 'd' + str(word_init_avg_norm) + 'init_' + \\\n","                   cell_type + '_' + \\\n","                   'cellNonL' + cell_non_lin + '_' +\\\n","                   'SENT' + sent_geom + '_' + \\\n","                   'INP' + inputs_geom + '_' + \\\n","                   'BIAS' + bias_geom + fix_biases_str + '_' + mat_str +\\\n","                   'FFNN' + ffnn_geom + str(before_mlr_dim) + ffnn_non_lin + '_' +\\\n","                   additional_features_str + \\\n","                   drp_str +\\\n","                   'MLR' + mlr_geom + '_' + \\\n","                   reg_beta_str + \\\n","                   hyp_opt_str + \\\n","                   c_str +\\\n","                   'prje' + str(PROJ_EPS) + '_' + \\\n","                   'bs' + str(batch_size) + '_' +\\\n","                   burnin_str +  '__' + now.strftime(\"%H:%M:%S%dM\")\n","\n","name_experiment = tensorboard_name\n","logger = util.setup_logger(name_experiment, logs_dir= os.path.join(root_path, 'logs/'), also_stdout=True)\n","logger.info('PARAMS :  ' + name_experiment)"],"execution_count":6,"outputs":[{"output_type":"stream","text":["2020/03/14 16:10:44: PARAMS :  _SNLI_W100d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp100id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__16:10:4414M\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"2zwJ_gTR7dsV","colab_type":"code","outputId":"1aef7bbe-59c9-47f9-ab89-f94bb6f43a1d","colab":{"base_uri":"https://localhost:8080/","height":610}},"source":["class HyperbolicRNNModel:\n","    def __init__(self, word_to_id, id_to_word, embeddings = None):\n","        self.word_to_id = word_to_id\n","        self.id_to_word = id_to_word\n","        self.embeddings_np = None if embeddings is None else np.array(embeddings)\n","\n","        self.construct_placeholders()\n","        self.construct_execution_graph()\n","\n","\n","    def construct_placeholders(self):\n","        self.label_placeholder = tf.placeholder(tf.int32,\n","                                                shape=[batch_size],\n","                                                name='label_placeholder')\n","\n","        self.word_ids_1 = tf.placeholder(tf.int32, shape=[batch_size, None],\n","                                         name='word_ids_1_placeholder')\n","        self.word_ids_2 = tf.placeholder(tf.int32, shape=[batch_size, None],\n","                                         name='word_ids_2_placeholder')\n","\n","        self.num_words_1 = tf.placeholder(tf.int32, shape=[batch_size],\n","                                          name='num_words_1_placeholder')\n","        self.num_words_2 = tf.placeholder(tf.int32, shape=[batch_size],\n","                                          name='num_words_2_placeholder')\n","\n","        self.burn_in_factor = tf.placeholder(dtype, name='burn_in_factor_placeholder')\n","        self.dropout_placeholder = tf.placeholder(dtype, name='dropout_placeholder')\n","\n","        if not (self.embeddings_np is None):\n","            self.embeddings_placeholder = tf.placeholder(dtype, shape = self.embeddings_np.shape,\n","                                                         name = 'embeddings_placeholder')\n","\n","    ###############################################################################################\n","    def construct_execution_graph(self):\n","\n","        # Collect vars separately. Word embeddings are not used here.\n","        eucl_vars = []\n","        hyp_vars = []\n","\n","        ################## word embeddings ###################\n","\n","        # Initialize word embeddings close to 0, to have average norm equal to word_init_avg_norm.\n","        if not (self.embeddings_np is None):\n","            self.embeddings = tf.get_variable(name = 'embeddings',\n","                                              dtype = dtype,\n","                                              shape = self.embeddings_np.shape,\n","                                              trainable = True,\n","                                              initializer = tf.constant_initializer(self.embeddings_np))\n","        else:\n","            maxval = (3. * (word_init_avg_norm ** 2) / (2. * word_dim)) ** (1. / 3)\n","            initializer = tf.random_uniform_initializer(minval=-maxval, maxval=maxval, dtype=dtype)\n","            self.embeddings = tf.get_variable('embeddings',\n","                                            dtype=dtype,\n","                                            shape=[len(self.word_to_id), word_dim],\n","                                            initializer=initializer)\n","\n","        if inputs_geom == 'eucl':\n","            eucl_vars += [self.embeddings]\n","\n","        ################## RNNs for sentence embeddings ###################\n","\n","        if cell_type == 'TFrnn':\n","            assert sent_geom == 'eucl'\n","            cell_class = lambda h_dim: tf.contrib.rnn.BasicRNNCell(h_dim)\n","        elif cell_type == 'TFgru':\n","            assert sent_geom == 'eucl'\n","            cell_class = lambda h_dim: tf.contrib.rnn.GRUCell(h_dim)\n","        elif cell_type == 'TFlstm':\n","            assert sent_geom == 'eucl'\n","            cell_class = lambda h_dim: tf.contrib.rnn.BasicLSTMCell(h_dim)\n","        elif cell_type == 'rnn' and sent_geom == 'eucl':\n","            cell_class = lambda h_dim: rnn_impl.EuclRNN(h_dim, dtype=dtype)\n","        elif cell_type == 'gru' and sent_geom == 'eucl':\n","            cell_class = lambda h_dim: rnn_impl.EuclGRU(h_dim, dtype=dtype)\n","        elif cell_type == 'rnn' and sent_geom == 'hyp':\n","            cell_class = lambda h_dim: rnn_impl.HypRNN(num_units=h_dim,\n","                                                       inputs_geom=inputs_geom,\n","                                                       bias_geom=bias_geom,\n","                                                       c_val=c_val,\n","                                                       non_lin=cell_non_lin,\n","                                                       fix_biases=fix_biases,\n","                                                       fix_matrices=fix_matrices,\n","                                                       matrices_init_eye=matrices_init_eye,\n","                                                       dtype=dtype)\n","        elif cell_type == 'gru' and sent_geom == 'hyp':\n","            cell_class = lambda h_dim: rnn_impl.HypGRU(num_units=h_dim,\n","                                                       inputs_geom=inputs_geom,\n","                                                       bias_geom=bias_geom,\n","                                                       c_val=c_val,\n","                                                       non_lin=cell_non_lin,\n","                                                       fix_biases=fix_biases,\n","                                                       fix_matrices=fix_matrices,\n","                                                       matrices_init_eye=matrices_init_eye,\n","                                                       dtype=dtype)\n","        else:\n","            logger.error('Not valid cell type: %s and sent_geom %s' % (cell_type, sent_geom))\n","            exit()\n","\n","        # RNN 1\n","        with tf.variable_scope(cell_type + '1'):\n","            word_embeddings_1 = tf.nn.embedding_lookup(self.embeddings, self.word_ids_1) # bs x num_w_s1 x dim\n","\n","            cell_1 = cell_class(hidden_dim)\n","            initial_state_1 = cell_1.zero_state(batch_size, dtype)\n","            outputs_1, state_1 = tf.nn.dynamic_rnn(cell=cell_1,\n","                                                   inputs=word_embeddings_1,\n","                                                   dtype=dtype,\n","                                                   initial_state=initial_state_1,\n","                                                   sequence_length=self.num_words_1)\n","            if cell_type == 'TFlstm':\n","                self.sent_1 = state_1[1]\n","            else:\n","                self.sent_1 = state_1\n","\n","\n","            sent1_norm = util.tf_norm(self.sent_1)\n","\n","\n","        # RNN 2\n","        with tf.variable_scope(cell_type + '2'):\n","            word_embeddings_2 = tf.nn.embedding_lookup(self.embeddings, self.word_ids_2)\n","            # tf.summary.scalar('word_emb2', tf.reduce_mean(tf.norm(word_embeddings_2, axis=2)))\n","\n","            cell_2 = cell_class(hidden_dim)\n","            initial_state_2 = cell_2.zero_state(batch_size, dtype)\n","            outputs_2, state_2 = tf.nn.dynamic_rnn(cell=cell_2,\n","                                                   inputs=word_embeddings_2,\n","                                                   dtype=dtype,\n","                                                   initial_state=initial_state_2,\n","                                                   sequence_length=self.num_words_2)\n","            if cell_type == 'TFlstm':\n","                self.sent_2 = state_2[1]\n","            else:\n","                self.sent_2 = state_2\n","\n","\n","            sent2_norm = util.tf_norm(self.sent_2)\n","\n","\n","        tf.summary.scalar('RNN/word_emb1', tf.reduce_mean(tf.norm(word_embeddings_1, axis=2)))\n","        tf.summary.scalar('RNN/sent1', tf.reduce_mean(sent1_norm))\n","        tf.summary.scalar('RNN/sent2', tf.reduce_mean(sent2_norm))\n","\n","\n","        eucl_vars += cell_1.eucl_vars + cell_2.eucl_vars\n","        if sent_geom == 'hyp':\n","            hyp_vars += cell_1.hyp_vars + cell_2.hyp_vars\n","\n"," \n","        ## Compute d(s1, s2)\n","        if sent_geom == 'eucl':\n","            d_sq_s1_s2 = util.tf_euclid_dist_sq(self.sent_1, self.sent_2)\n","        else:\n","            d_sq_s1_s2 = util.tf_poinc_dist_sq(self.sent_1, self.sent_2, c = c_val)\n","\n","\n","        ##### Some summaries:\n","\n","        # For summaries and debugging, we need these:\n","        pos_labels = tf.reshape(tf.cast(self.label_placeholder, tf.float64), [-1, 1])\n","        neg_labels = 1. - pos_labels\n","        weights_pos_labels = pos_labels / tf.reduce_sum(pos_labels)\n","        weights_neg_labels = neg_labels / tf.reduce_sum(neg_labels)\n","\n","        ################## first feed forward layer ###################\n","\n","        # Define variables for the first feed-forward layer: W1 * s1 + W2 * s2 + b + bd * d(s1,s2)\n","        W_ff_s1 = tf.get_variable('W_ff_s1',\n","                                  dtype=dtype,\n","                                  shape=[hidden_dim, before_mlr_dim],\n","                                  initializer= tf.contrib.layers.xavier_initializer())\n","\n","        W_ff_s2 = tf.get_variable('W_ff_s2',\n","                                  dtype=dtype,\n","                                  shape=[hidden_dim, before_mlr_dim],\n","                                  initializer= tf.contrib.layers.xavier_initializer())\n","\n","        b_ff = tf.get_variable('b_ff',\n","                               dtype=dtype,\n","                               shape=[1, before_mlr_dim],\n","                               initializer=tf.constant_initializer(0.0))\n","\n","        b_ff_d = tf.get_variable('b_ff_d',\n","                                 dtype=dtype,\n","                                 shape=[1, before_mlr_dim],\n","                                 initializer=tf.constant_initializer(0.0))\n","\n","        eucl_vars += [W_ff_s1, W_ff_s2]\n","        if ffnn_geom == 'eucl' or bias_geom == 'eucl':\n","            eucl_vars += [b_ff]\n","            if additional_features == 'dsq':\n","                eucl_vars += [b_ff_d]\n","        else:\n","            hyp_vars += [b_ff]\n","            if additional_features == 'dsq':\n","                hyp_vars += [b_ff_d]\n","\n","\n","        if ffnn_geom == 'eucl' and sent_geom == 'hyp': # Sentence embeddings are Euclidean after log, except the proper distance (Eucl or hyp) is kept!\n","            self.sent_1 = util.tf_log_map_zero(self.sent_1, c_val)\n","            self.sent_2 = util.tf_log_map_zero(self.sent_2, c_val)\n","\n","        ####### Build output_ffnn #######\n","        if ffnn_geom == 'eucl':\n","            output_ffnn = tf.matmul(self.sent_1, W_ff_s1) + tf.matmul(self.sent_2, W_ff_s2) + b_ff\n","            if additional_features == 'dsq': # [u, v, d(u,v)^2]\n","                output_ffnn = output_ffnn + d_sq_s1_s2 * b_ff_d\n","\n","        else:\n","            assert sent_geom == 'hyp'\n","            ffnn_s1 = util.tf_mob_mat_mul(W_ff_s1, self.sent_1, c_val)\n","            ffnn_s2 = util.tf_mob_mat_mul(W_ff_s2, self.sent_2, c_val)\n","            output_ffnn = util.tf_mob_add(ffnn_s1, ffnn_s2, c_val)\n","\n","            hyp_b_ff = b_ff\n","            if bias_geom == 'eucl':\n","                hyp_b_ff = util.tf_exp_map_zero(b_ff, c_val)\n","            output_ffnn = util.tf_mob_add(output_ffnn, hyp_b_ff, c_val)\n","\n","            if additional_features == 'dsq': # [u, v, d(u,v)^2]\n","                hyp_b_ff_d = b_ff_d\n","                if bias_geom == 'eucl':\n","                    hyp_b_ff_d = util.tf_exp_map_zero(b_ff_d, c_val)\n","\n","                output_ffnn = util.tf_mob_add(output_ffnn,\n","                                              util.tf_mob_scalar_mul(d_sq_s1_s2, hyp_b_ff_d, c_val),\n","                                              c_val)\n","\n","        if ffnn_geom == 'eucl':\n","            output_ffnn = util.tf_eucl_non_lin(output_ffnn, non_lin=ffnn_non_lin)\n","        else:\n","            output_ffnn = util.tf_hyp_non_lin(output_ffnn,\n","                                              non_lin=ffnn_non_lin,\n","                                              hyp_output = (mlr_geom == 'hyp' and dropout == 1.0),\n","                                              c=c_val)\n","        # Mobius dropout\n","        if dropout < 1.0:\n","            # If we are here, then output_ffnn should be Euclidean.\n","            output_ffnn = tf.nn.dropout(output_ffnn, keep_prob=self.dropout_placeholder)\n","            if (mlr_geom == 'hyp'):\n","                output_ffnn = util.tf_exp_map_zero(output_ffnn, c_val)\n","\n","\n","        ################## MLR ###################\n","        # output_ffnn is batch_size x before_mlr_dim\n","\n","        A_mlr = []\n","        P_mlr = []\n","        logits_list = []\n","        for cl in range(num_classes):\n","            A_mlr.append(tf.get_variable('A_mlr' + str(cl),\n","                                         dtype=dtype,\n","                                         shape=[1, before_mlr_dim],\n","                                         initializer=tf.contrib.layers.xavier_initializer()))\n","            eucl_vars += [A_mlr[cl]]\n","\n","            P_mlr.append(tf.get_variable('P_mlr' + str(cl),\n","                                         dtype=dtype,\n","                                         shape=[1, before_mlr_dim],\n","                                         initializer=tf.constant_initializer(0.0)))\n","\n","            if mlr_geom == 'eucl':\n","                eucl_vars += [P_mlr[cl]]\n","                logits_list.append(tf.reshape(util.tf_dot(-P_mlr[cl] + output_ffnn, A_mlr[cl]), [-1]))\n","\n","            elif mlr_geom == 'hyp':\n","                hyp_vars += [P_mlr[cl]]\n","                minus_p_plus_x = util.tf_mob_add(-P_mlr[cl], output_ffnn, c_val)\n","                norm_a = util.tf_norm(A_mlr[cl])\n","                lambda_px = util.tf_lambda_x(minus_p_plus_x, c_val)\n","                px_dot_a = util.tf_dot(minus_p_plus_x, tf.nn.l2_normalize(A_mlr[cl]))\n","                logit = 2. / np.sqrt(c_val) * norm_a * tf.asinh(np.sqrt(c_val) * px_dot_a * lambda_px)\n","                logits_list.append(tf.reshape(logit, [-1]))\n","\n","        self.logits = tf.stack(logits_list, axis=1)\n","\n","        self.argmax_idx = tf.argmax(self.logits, axis=1, output_type=tf.int32)\n","\n","        self.loss = tf.reduce_mean(\n","            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_placeholder,\n","                                                           logits=self.logits))\n","        tf.summary.scalar('classif/unreg_loss', self.loss)\n","\n","        if reg_beta > 0.0:\n","            assert num_classes == 2\n","            distance_regularizer = tf.reduce_mean(\n","                (tf.cast(self.label_placeholder, dtype=dtype) - 0.5) * d_sq_s1_s2)\n","\n","            self.loss = self.loss + reg_beta * distance_regularizer\n","\n","        self.acc = tf.reduce_mean(tf.to_float(tf.equal(self.argmax_idx, self.label_placeholder)))\n","        tf.summary.scalar('classif/accuracy', self.acc)\n","\n","\n","\n","\n","        ######################################## OPTIMIZATION ######################################\n","        all_updates_ops = []\n","\n","        ###### Update Euclidean parameters using Adam.\n","        optimizer_euclidean_params = tf.train.AdamOptimizer(learning_rate=1e-3)\n","        eucl_grads = optimizer_euclidean_params.compute_gradients(self.loss, eucl_vars)\n","        capped_eucl_gvs = [(tf.clip_by_norm(grad, 1.), var) for grad, var in eucl_grads]  ###### Clip gradients\n","        all_updates_ops.append(optimizer_euclidean_params.apply_gradients(capped_eucl_gvs))\n","\n","\n","        ###### Update Hyperbolic parameters, i.e. word embeddings and some biases in our case.\n","        def rsgd(v, riemannian_g, learning_rate):\n","            if hyp_opt == 'rsgd':\n","                return util.tf_exp_map_x(v, -self.burn_in_factor * learning_rate * riemannian_g, c=c_val)\n","            else:\n","                # Use approximate RSGD based on a simple retraction.\n","                updated_v = v - self.burn_in_factor * learning_rate * riemannian_g\n","                # Projection op after SGD update. Need to make sure embeddings are inside the unit ball.\n","                return util.tf_project_hyp_vecs(updated_v, c_val)\n","\n","\n","        if inputs_geom == 'hyp':\n","            grads_and_indices_hyp_words = tf.gradients(self.loss, self.embeddings)\n","            grads_hyp_words = grads_and_indices_hyp_words[0].values\n","            repeating_indices = grads_and_indices_hyp_words[0].indices\n","            unique_indices, idx_in_repeating_indices = tf.unique(repeating_indices)\n","            agg_gradients = tf.unsorted_segment_sum(grads_hyp_words,\n","                                                    idx_in_repeating_indices,\n","                                                    tf.shape(unique_indices)[0])\n","\n","            agg_gradients = tf.clip_by_norm(agg_gradients, 1.) ######## Clip gradients\n","            unique_word_emb = tf.nn.embedding_lookup(self.embeddings, unique_indices)  # no repetitions here\n","\n","            riemannian_rescaling_factor = util.riemannian_gradient_c(unique_word_emb, c=c_val)\n","            rescaled_gradient = riemannian_rescaling_factor * agg_gradients\n","\n","            all_updates_ops.append(tf.scatter_update(self.embeddings,\n","                                                     unique_indices,\n","                                                     rsgd(unique_word_emb, rescaled_gradient, lr_words))) # Updated rarely\n","\n","        if len(hyp_vars) > 0:\n","            hyp_grads = tf.gradients(self.loss, hyp_vars)\n","            capped_hyp_grads = [tf.clip_by_norm(grad, 1.) for grad in hyp_grads]  ###### Clip gradients\n","\n","\n","            for i in range(len(hyp_vars)):\n","                riemannian_rescaling_factor = util.riemannian_gradient_c(hyp_vars[i], c=c_val)\n","                rescaled_gradient = riemannian_rescaling_factor * capped_hyp_grads[i]\n","                all_updates_ops.append(tf.assign(hyp_vars[i], rsgd(hyp_vars[i], rescaled_gradient, lr_ffnn)))  # Updated frequently\n","\n","        self.all_optimizer_var_updates_op = tf.group(*all_updates_ops)\n","\n","\n","        self.summary_merged = tf.summary.merge_all()\n","        self.test_summary_writer = tf.summary.FileWriter(\n","            os.path.join(root_path, 'tb_28may/' + tensorboard_name + '/'))\n","\n","\n","    ###############################################################################################\n","    ###############################################################################################\n","    ###############################################################################################\n","    def test(self, sess, test_or_valid_data, name, summary_i):\n","        N = len(test_or_valid_data)\n","        i = 0\n","        predictions = []\n","\n","        avg_loss = 0.0\n","        num_b_in_loss = 0\n","        while i < N:\n","            batch_word_ids_1, batch_num_words_1, batch_word_ids_2, batch_num_words_2, batch_label = self.next_batch(\n","                i=i, N=N, data=test_or_valid_data)\n","\n","            if name == 'test':\n","                summary, loss, argmax_idx = \\\n","                    sess.run([self.summary_merged, self.loss, self.argmax_idx], feed_dict={\n","                        self.word_ids_1: batch_word_ids_1,\n","                        self.num_words_1: batch_num_words_1,\n","                        self.word_ids_2: batch_word_ids_2,\n","                        self.num_words_2: batch_num_words_2,\n","                        self.label_placeholder: batch_label,\n","                        self.dropout_placeholder: 1.0\n","                    })\n","                self.test_summary_writer.add_summary(summary, summary_i)\n","\n","            else:\n","                loss, argmax_idx = \\\n","                    sess.run([self.loss, self.argmax_idx], feed_dict={\n","                        self.word_ids_1: batch_word_ids_1,\n","                        self.num_words_1: batch_num_words_1,\n","                        self.word_ids_2: batch_word_ids_2,\n","                        self.num_words_2: batch_num_words_2,\n","                        self.label_placeholder: batch_label,\n","                        self.dropout_placeholder: 1.0\n","                    })\n","\n","            avg_loss += loss\n","            num_b_in_loss += 1\n","\n","            for ci in argmax_idx:\n","                predictions.append(ci)\n","\n","            i += batch_size\n","\n","        avg_loss /= num_b_in_loss\n","\n","        num_correct = 0.0\n","        glE_predE = 0.0\n","        glN_predN = 0.0\n","        glE_predN = 0.0\n","        glN_predE = 0.0\n","\n","        predictions = predictions[:N]\n","\n","        for ind, predicted_label in enumerate(predictions):\n","            gold_label = test_or_valid_data[ind][4]\n","\n","            if predicted_label == gold_label:\n","                num_correct += 1.0\n","                if predicted_label == 1:\n","                    glE_predE += 1.0\n","                else:\n","                    glN_predN += 1.0\n","            else:\n","                if predicted_label == 1:\n","                    glN_predE += 1.0\n","                else:\n","                    glE_predN += 1.0\n","\n","        accuracy = num_correct / (1.0 * N)\n","\n","        if name == 'test':\n","            logger.info('For ' + name + ': ==> ' +\n","                        ' glN_predN = ' + str(glN_predN) + '; glE_predN = ' + str(glE_predN) +\n","                        '; glN_predE = ' + str(glN_predE) + '; glE_predE = ' + str(glE_predE))\n","\n","        return accuracy\n","\n","\n","    def next_batch(self, i, N, data):\n","        it = i\n","        to = min(i + batch_size, N)\n","\n","        MAX_PAD = 0\n","        batch_word_ids_1 = []\n","        batch_num_words_1 = []\n","\n","        batch_word_ids_2 = []\n","        batch_num_words_2 = []\n","\n","        batch_label = []\n","\n","        while it < to:\n","            word_ids_1 = data[it][0]\n","            num_words_1 = data[it][1]\n","            word_ids_2 = data[it][2]\n","            num_words_2 = data[it][3]\n","            label = data[it][4]\n","\n","            MAX_PAD = max(MAX_PAD, max(len(word_ids_1), len(word_ids_2)))\n","\n","            batch_word_ids_1.append(word_ids_1)\n","            batch_num_words_1.append(num_words_1)\n","\n","            batch_word_ids_2.append(word_ids_2)\n","            batch_num_words_2.append(num_words_2)\n","\n","            batch_label.append(label)\n","\n","            it += 1\n","\n","            if it == to and to == N:\n","                it = 0\n","                to = batch_size - len(batch_word_ids_1)\n","\n","        for ind in range(0, batch_size):\n","\n","            while len(batch_word_ids_1[ind]) < MAX_PAD:\n","                batch_word_ids_1[ind].append(0)\n","\n","            while len(batch_word_ids_2[ind]) < MAX_PAD:\n","                batch_word_ids_2[ind].append(0)\n","\n","            if len(batch_word_ids_1[ind]) != MAX_PAD or len(batch_word_ids_2[ind]) != MAX_PAD:\n","                logger.error('THIS IS WRONG!: len1: %d\\nlen2: %d\\nMAX_PAD:%d\\n' %\n","                             (len(batch_word_ids_1[ind]), len(batch_word_ids_2[ind]), MAX_PAD))\n","                exit()\n","\n","        batch_word_ids_1 = np.array(batch_word_ids_1)\n","        batch_num_words_1 = np.array(batch_num_words_1)\n","\n","        batch_word_ids_2 = np.array(batch_word_ids_2)\n","        batch_num_words_2 = np.array(batch_num_words_2)\n","        batch_label = np.array(batch_label)\n","\n","        return batch_word_ids_1, batch_num_words_1, batch_word_ids_2, batch_num_words_2, batch_label\n","\n","\n","    def dataset_to_minibatches(self, dataset):\n","        N = len(dataset)\n","        i = 0\n","        batches_list = []\n","        while i < N:\n","            batches_list.append(self.next_batch(i=i, N=N, data=dataset))\n","            i += batch_size\n","        return batches_list\n","\n","\n","    def train(self, training_data, dev_data, test_data, restore_model=False, save_model=False,\n","              restore_from_path=None, save_to_path=None):\n","\n","        training_data_batches = self.dataset_to_minibatches(training_data)\n","        num_batches = len(training_data_batches)\n","        N = len(training_data)\n","\n","        saver = tf.train.Saver()\n","        best_test_accuracy = 0.0\n","        best_validation_accuracy = 0.0\n","        best_i = 0\n","\n","        PRINT_STEP = 2000\n","\n","        config = tf.ConfigProto(\n","            intra_op_parallelism_threads=5,\n","            inter_op_parallelism_threads=3,\n","            log_device_placement=True\n","        )\n","        config.gpu_options.allow_growth = True\n","        with tf.Session(config=config) as sess:\n","\n","            if restore_model:\n","                saver.restore(sess, restore_from_path)\n","            else:\n","                sess.run(tf.global_variables_initializer())\n","\n","            sess.graph.finalize()\n","\n","            burn_in_factor = 1.0\n","            epoch = -1\n","            while epoch < num_epochs:\n","                epoch += 1\n","                logger.info('Epoch: %d' % epoch)\n","\n","                cur_total_time = 0\n","                shuffle(training_data_batches) # Shuffle training data after each epoch.\n","\n","                if burnin and epoch == 0:\n","                    burn_in_factor /= 10.0\n","                i = -1\n","                while i < num_batches - 1:\n","                    i += 1\n","\n","                    # Training\n","                    batch_word_ids_1, batch_num_words_1, \\\n","                    batch_word_ids_2, batch_num_words_2, \\\n","                    batch_label = training_data_batches[i]\n","\n","                    feed_dict = {\n","                        self.word_ids_1: batch_word_ids_1,\n","                        self.num_words_1: batch_num_words_1,\n","                        self.word_ids_2: batch_word_ids_2,\n","                        self.num_words_2: batch_num_words_2,\n","                        self.label_placeholder: batch_label,\n","                        self.burn_in_factor: burn_in_factor,\n","                        self.dropout_placeholder: dropout\n","                    }\n","\n","                    sess_time_start = time.time()\n","                    curr_loss, _ = sess.run([self.loss, self.all_optimizer_var_updates_op], feed_dict=feed_dict)\n","                    cur_total_time += time.time() - sess_time_start\n","\n","                    if i % PRINT_STEP == 0:\n","                        if i > 0:\n","                            avg_sec_per_sent = cur_total_time / (PRINT_STEP * batch_size)\n","                            logger.info('Num examples processed: %d. curr_loss: %.4f; sec_per_sent: %.4f' % (\n","                                epoch * N + i * batch_size, curr_loss, avg_sec_per_sent))\n","                        cur_total_time = 0\n","\n","\n","                        # Testing\n","                        validation_accuracy = self.test(sess,\n","                                                        dev_data,\n","                                                        'validation',\n","                                                        epoch * N + i * batch_size)\n","                        test_accuracy = self.test(sess,\n","                                                  test_data,\n","                                                  'test',\n","                                                  epoch * N + i * batch_size)\n","                        logger.info('CURRENT val accuracy: %.4f ; test accuracy: \\033[92m %.4f \\033[0m' %\n","                                    (validation_accuracy, test_accuracy))\n","\n","                        if validation_accuracy > best_validation_accuracy:\n","                            best_validation_accuracy = validation_accuracy\n","                            best_test_accuracy = test_accuracy\n","                            best_i = epoch * N + i * batch_size\n","\n","                        logger.info(('BEST: i = %d, val acc: ' + '\\033[94m' + ' %.2f' + '\\033[0m' +\n","                                     ', test acc: ' + '\\033[91m' + ' %.2f' + '\\033[0m') %\n","                                    (best_i, 100 * best_validation_accuracy, 100 * best_test_accuracy))\n","\n","                        if save_model:\n","                            store_time_begin = time.time()\n","                            saver.save(sess, '%s_epoch_%d_it_%d.ckpt' % (save_to_path, epoch, i))\n","                            logger.info('Stored the model in %d seconds.' %\n","                                        (time.time() - store_time_begin))\n","\n","                        logger.info('EXPERIMENT = ' + name_experiment)\n","                        logger.info('=============================================================')\n","\n","                    if np.isinf(curr_loss) or np.isnan(curr_loss):\n","                        logger.error('At example ' + str(epoch * N + i * batch_size) +\n","                                     '; curr_loss: ' + str(curr_loss))\n","                        exit()\n","\n","\n","        logger.info(('DONE -- BEST: i = %d, val acc: ' + '\\033[94m' + ' %.2f' + '\\033[0m' +\n","                     ', test acc: ' + '\\033[91m' + ' %.2f' + '\\033[0m') %\n","                    (best_i, 100 * best_validation_accuracy, 100 * best_test_accuracy))\n","\n","\n","def get_datasets():\n","    logger.info('Loading train - val - test data')\n","\n","    test_data = pickle.load(open(test_data_file_path, 'rb'))\n","    dev_data = pickle.load(open(dev_data_file_path, 'rb'))\n","    training_data = pickle.load(open(training_data_file_path, 'rb'))\n","\n","    logger.info('Training data size: %d' % len(training_data))\n","    class_to_count = {1: 0.0, 0: 0.0}\n","    for i in range(len(training_data)):\n","        class_to_count[training_data[i][4]] += 1.0\n","    for cl in class_to_count:\n","        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(training_data)))\n","\n","    logger.info('Validation data size: %d' % len(dev_data))\n","    class_to_count = {1: 0.0, 0: 0.0}\n","    for i in range(len(test_data)):\n","        class_to_count[test_data[i][4]] += 1.0\n","    for cl in class_to_count:\n","        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(dev_data)))\n","\n","    logger.info('Test data size: %d' % len(test_data))\n","    class_to_count = {1: 0.0, 0: 0.0}\n","    for i in range(len(test_data)):\n","        class_to_count[test_data[i][4]] += 1.0\n","    for cl in class_to_count:\n","        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(test_data)))\n","\n","    return training_data, dev_data, test_data\n","\n","\n","def run():\n","    word_to_id = pickle.load(open(word_to_id_file_path, 'rb'))\n","    id_to_word = pickle.load(open(id_to_word_file_path, 'rb'))\n","    embedds = None if not pretrained else pickle.load(open(pretrained_path, 'rb'))\n","    model = HyperbolicRNNModel(word_to_id=word_to_id,\n","                               id_to_word=id_to_word,\n","                               embeddings = embedds)\n","\n","    training_data, dev_data, test_data = get_datasets()\n","\n","    model.train(training_data=training_data,\n","                dev_data=dev_data,\n","                test_data=test_data,\n","                save_model=save_model,\n","                save_to_path=save_to_path,\n","                restore_model=restore_model,\n","                restore_from_path=restore_from_path)\n","\n","if __name__=='__main__':\n","  run()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-10-bcffb3cdec13>:109: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","Init RNN cell\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Init RNN cell\n","WARNING:tensorflow:From <ipython-input-10-bcffb3cdec13>:291: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n"],"name":"stdout"},{"output_type":"stream","text":["2020/03/14 16:23:29: Loading train - val - test data\n","2020/03/14 16:23:33: Training data size: 549367\n","2020/03/14 16:23:33: Class 1 has 33.3868 percent samples\n","2020/03/14 16:23:33: Class 0 has 66.6132 percent samples\n","2020/03/14 16:23:33: Validation data size: 9842\n","2020/03/14 16:23:33: Class 1 has 34.2207 percent samples\n","2020/03/14 16:23:33: Class 0 has 65.5964 percent samples\n","2020/03/14 16:23:33: Test data size: 9824\n","2020/03/14 16:23:33: Class 1 has 34.2834 percent samples\n","2020/03/14 16:23:33: Class 0 has 65.7166 percent samples\n"],"name":"stderr"},{"output_type":"stream","text":["Device mapping:\n","/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n","/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n","/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\n","\n"],"name":"stdout"},{"output_type":"stream","text":["2020/03/14 16:23:47: Epoch: 0\n","2020/03/14 16:24:17: For test: ==>  glN_predN = 5136.0; glE_predN = 2680.0; glN_predE = 1320.0; glE_predE = 688.0\n","2020/03/14 16:24:17: CURRENT val accuracy: 0.6004 ; test accuracy: \u001b[92m 0.5928 \u001b[0m\n","2020/03/14 16:24:17: BEST: i = 0, val acc: \u001b[94m 60.04\u001b[0m, test acc: \u001b[91m 59.28\u001b[0m\n","2020/03/14 16:24:18: Stored the model in 0 seconds.\n","2020/03/14 16:24:18: EXPERIMENT = _SNLI_W100d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp100id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__16:10:4414M\n","2020/03/14 16:24:18: =============================================================\n"],"name":"stderr"}]}]}