{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Get_Embeddings.ipynb","provenance":[{"file_id":"1zv3p8Fya0MBzFPHlyGShZ_QLy5ASciE1","timestamp":1584376405522},{"file_id":"137gEQPNNDQGbc07puwjg8vdwOIVjOMW0","timestamp":1584111543199}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qLMTdd5m49Eq","colab_type":"text"},"source":["### Environment Setup"]},{"cell_type":"markdown","metadata":{"id":"V-0rHFza5Nkl","colab_type":"text"},"source":["##### GPU config"]},{"cell_type":"code","metadata":{"id":"_d-kNkHa4-px","colab_type":"code","outputId":"47daef37-b217-48a2-ecc7-4dc00e49f479","executionInfo":{"status":"ok","timestamp":1585575155980,"user_tz":-60,"elapsed":1926,"user":{"displayName":"Seamus O'Keeffe","photoUrl":"","userId":"11106926260732909747"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"\n","Runtime -> Change runtime type -> Hardware Accelorator (change from None to GPU)\n","Make sure there is no assertion error below\n","\"\"\"\n","%tensorflow_version 1.x\n","import tensorflow as tf\n","#assert tf.test.gpu_device_name() != '', \"NO GPU DETECTED\"\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["TensorFlow 1.x selected.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GLUoFdTf5QKM","colab_type":"text"},"source":["gdrive mount & add project directory"]},{"cell_type":"code","metadata":{"id":"-cao7b1U17S3","colab_type":"code","outputId":"66da96c3-6263-4431-e22a-6dd39d9d32a5","executionInfo":{"status":"ok","timestamp":1585575155982,"user_tz":-60,"elapsed":1918,"user":{"displayName":"Seamus O'Keeffe","photoUrl":"","userId":"11106926260732909747"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["\"\"\"\n","Upload hyperbolic_nn folder to your Google drive root directory and run the following.\n","Authentication (in-browser) is required\n","\"\"\"\n","from google.colab import drive\n","drive.mount('/content/gdrive')\n","\n","import sys\n","sys.path.append('/content/gdrive/My Drive/hyperbolic_nn')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"QBGnV7o05mHh","colab_type":"text"},"source":["## Experiment"]},{"cell_type":"code","metadata":{"id":"ih7w4zo64TYs","colab_type":"code","outputId":"f9abc18e-7971-4b23-9396-108d6ffb4d8f","executionInfo":{"status":"ok","timestamp":1585575155983,"user_tz":-60,"elapsed":1912,"user":{"displayName":"Seamus O'Keeffe","photoUrl":"","userId":"11106926260732909747"}},"colab":{"base_uri":"https://localhost:8080/","height":70}},"source":["import os\n","import numpy as np\n","import pickle\n","import time\n","import random\n","from random import shuffle\n","import math\n","from datetime import datetime\n","import matplotlib.pyplot as plt\n","\n","\n","import util\n","import rnn_impl"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /content/gdrive/My Drive/hyperbolic_nn/rnn_impl.py:4: The name tf.nn.rnn_cell.RNNCell is deprecated. Please use tf.compat.v1.nn.rnn_cell.RNNCell instead.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"0wvCzLengbnt","colab_type":"code","colab":{}},"source":["epoch = 6\n","\n","### arguments to change ###\n","\"\"\"\n","num_epochs: int \n","word_dim: int\n","cell_type: 'rnn/gru/TFrnn/TFgru/TFlstm\n","geom: 'eucl/hyp'\n","hyp_opt: 'rsgd/projsgd\n","\"\"\"\n","base_name = \"experiment_2d_hyp_rnn_rsgd\"\n","root_path = '/content/gdrive/My Drive/hyperbolic_nn'\n","num_epochs = 6\n","\n","is_spelling_corrected = True\n","\n","word_dim = 2\n","hidden_dim = word_dim\n","\n","cell_type = 'rnn'\n","geom = 'hyp'\n","hyp_opt = \"rsgd\"\n","\n","lr_ffnn = 0.01\n","lr_words = 0.1\n","batch_size = 64\n","\n","save_model = False\n","save_to_path = root_path+\"/models/\"+base_name\n","\n","# Stopped at epoch 8\n","restore_model = True\n","restore_from_path = root_path +  \"/2d_embedds/Rand_in_all_epochs_2/experiment_2d_hyp_rnn_rsgd_pretrain_poincare_glove_50_epoch_epoch_\"+str(epoch)+\"_it_0.ckpt\"\n","\n","pretrained = False\n","pretrained_path = root_path+\"/Poincare_glove/2d_hyp_snli_embedds_poincare_glove\"\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"F4lHnWLB6b58","colab_type":"code","outputId":"e6a1bc86-368a-4af4-f40e-4a12d69ea227","executionInfo":{"status":"ok","timestamp":1585575155984,"user_tz":-60,"elapsed":1893,"user":{"displayName":"Seamus O'Keeffe","photoUrl":"","userId":"11106926260732909747"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["### No need to change below ###\n","dtype = tf.float64\n","\n","# Project setup\n","num_classes = 2\n","\n","if is_spelling_corrected:\n","  snli_data_type = 'clean'\n","else:\n","  snli_data_type = 'dirty'\n","\n","word_to_id_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type, 'word_to_id')\n","id_to_word_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type,'id_to_word')\n","suffix = '_' + str(num_classes) + 'class'\n","training_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'train' + suffix)\n","test_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'test' + suffix)\n","dev_data_file_path = os.path.join(root_path, 'snli_dataset/', snli_data_type ,'dev' + suffix)\n","\n","PROJ_EPS = 1e-5\n","util.PROJ_EPS = PROJ_EPS\n","dataset = \"SNLI\"\n","c_val = 1.0\n","\n","# FFNN params: id/relu/tanh/sigmoid\n","cell_non_lin = 'id'\n","ffnn_non_lin = 'id'\n","\n","word_init_avg_norm = 0.001\n","additional_features = \"dsq\"\n","\n","dropout = 1.0\n","\n","sent_geom = geom\n","inputs_geom = geom\n","bias_geom = geom\n","\n","ffnn_geom = geom\n","mlr_geom = geom\n","\n","before_mlr_dim = word_dim\n","\n","fix_biases = 'n'\n","fix_biases_str = ''\n","fix_matrices = 'n'\n","matrices_init_eye = 'n'\n","mat_str = ''\n","\n","# Optimization params\n","burnin = 'n'\n","\n","# L2 regularization\n","reg_beta = 0.0\n","\n","\n","if inputs_geom == 'hyp' or bias_geom == 'hyp' or ffnn_geom =='hyp' or mlr_geom == 'hyp':\n","    hyp_opt_str = hyp_opt + '_lrW' + str(lr_words) + '_lrFF' + str(lr_ffnn) + '_'\n","else:\n","    hyp_opt_str = ''\n","\n","if c_val != 1.0:\n","    c_str = 'C'  + str(c_val) + '_'\n","else:\n","    c_str = ''\n","\n","if dropout != 1.0:\n","    drp_str = 'drp' + str(dropout) + '_'\n","else:\n","    drp_str = ''\n","\n","burnin_str = ''\n","if burnin:\n","    burnin_str = 'burn' + str(burnin).lower()\n","\n","reg_beta_str = ''\n","if reg_beta > 0.0:\n","    reg_beta_str = 'reg' + str(reg_beta) + '_'\n","\n","additional_features_str = additional_features\n","if additional_features != '':\n","    additional_features_str = additional_features + '_'\n","\n","now = datetime.now()\n","\n","tensorboard_name = base_name + '_' +\\\n","                   dataset + '_' +\\\n","                   'W' + str(word_dim) + 'd' + str(word_init_avg_norm) + 'init_' + \\\n","                   cell_type + '_' + \\\n","                   'cellNonL' + cell_non_lin + '_' +\\\n","                   'SENT' + sent_geom + '_' + \\\n","                   'INP' + inputs_geom + '_' + \\\n","                   'BIAS' + bias_geom + fix_biases_str + '_' + mat_str +\\\n","                   'FFNN' + ffnn_geom + str(before_mlr_dim) + ffnn_non_lin + '_' +\\\n","                   additional_features_str + \\\n","                   drp_str +\\\n","                   'MLR' + mlr_geom + '_' + \\\n","                   reg_beta_str + \\\n","                   hyp_opt_str + \\\n","                   c_str +\\\n","                   'prje' + str(PROJ_EPS) + '_' + \\\n","                   'bs' + str(batch_size) + '_' +\\\n","                   burnin_str +  '__' + now.strftime(\"%H:%M:%S%dM\")\n","\n","name_experiment = tensorboard_name\n","logger = util.setup_logger(datetime.now().strftime(\"%Y%m%d-%H%:%M\")+name_experiment+'.txt', logs_dir= os.path.join(root_path, 'logs/'), also_stdout=True)\n","logger.info('PARAMS :  ' + name_experiment)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["2020/03/30 13:32:36: PARAMS :  experiment_2d_hyp_rnn_rsgd_SNLI_W2d0.001init_rnn_cellNonLid_SENThyp_INPhyp_BIAShyp_FFNNhyp2id_dsq_MLRhyp_rsgd_lrW0.1_lrFF0.01_prje1e-05_bs64_burnn__13:32:3630M\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"eCaq8q9OM5s9","colab_type":"code","outputId":"630acc2d-1428-4b4e-bd08-1757e5884e64","executionInfo":{"status":"ok","timestamp":1585575178971,"user_tz":-60,"elapsed":24870,"user":{"displayName":"Seamus O'Keeffe","photoUrl":"","userId":"11106926260732909747"}},"colab":{"base_uri":"https://localhost:8080/","height":675}},"source":["class HyperbolicRNNModel:\n","    def __init__(self, word_to_id, id_to_word):\n","        self.word_to_id = word_to_id\n","        self.id_to_word = id_to_word\n","\n","        self.construct_placeholders()\n","        self.construct_execution_graph()\n","\n","\n","    def construct_placeholders(self):\n","        self.label_placeholder = tf.placeholder(tf.int32,\n","                                                shape=[batch_size],\n","                                                name='label_placeholder')\n","\n","        self.word_ids_1 = tf.placeholder(tf.int32, shape=[batch_size, None],\n","                                         name='word_ids_1_placeholder')\n","        self.word_ids_2 = tf.placeholder(tf.int32, shape=[batch_size, None],\n","                                         name='word_ids_2_placeholder')\n","\n","        self.num_words_1 = tf.placeholder(tf.int32, shape=[batch_size],\n","                                          name='num_words_1_placeholder')\n","        self.num_words_2 = tf.placeholder(tf.int32, shape=[batch_size],\n","                                          name='num_words_2_placeholder')\n","\n","        self.burn_in_factor = tf.placeholder(dtype, name='burn_in_factor_placeholder')\n","        self.dropout_placeholder = tf.placeholder(dtype, name='dropout_placeholder')\n","\n","\n","    ###############################################################################################\n","    def construct_execution_graph(self):\n","\n","        # Collect vars separately. Word embeddings are not used here.\n","        eucl_vars = []\n","        hyp_vars = []\n","\n","        ################## word embeddings ###################\n","\n","        # Initialize word embeddings close to 0, to have average norm equal to word_init_avg_norm.\n","        maxval = (3. * (word_init_avg_norm ** 2) / (2. * word_dim)) ** (1. / 3)\n","        initializer = tf.random_uniform_initializer(minval=-maxval, maxval=maxval, dtype=dtype)\n","        self.embeddings = tf.get_variable('embeddings',\n","                                          dtype=dtype,\n","                                          shape=[len(self.word_to_id), word_dim],\n","                                          initializer=initializer)\n","\n","        if inputs_geom == 'eucl':\n","            eucl_vars += [self.embeddings]\n","\n","        ################## RNNs for sentence embeddings ###################\n","\n","        if cell_type == 'TFrnn':\n","            assert sent_geom == 'eucl'\n","            cell_class = lambda h_dim: tf.contrib.rnn.BasicRNNCell(h_dim)\n","        elif cell_type == 'TFgru':\n","            assert sent_geom == 'eucl'\n","            cell_class = lambda h_dim: tf.contrib.rnn.GRUCell(h_dim)\n","        elif cell_type == 'TFlstm':\n","            assert sent_geom == 'eucl'\n","            cell_class = lambda h_dim: tf.contrib.rnn.BasicLSTMCell(h_dim)\n","        elif cell_type == 'rnn' and sent_geom == 'eucl':\n","            cell_class = lambda h_dim: rnn_impl.EuclRNN(h_dim, dtype=dtype)\n","        elif cell_type == 'gru' and sent_geom == 'eucl':\n","            cell_class = lambda h_dim: rnn_impl.EuclGRU(h_dim, dtype=dtype)\n","        elif cell_type == 'rnn' and sent_geom == 'hyp':\n","            cell_class = lambda h_dim: rnn_impl.HypRNN(num_units=h_dim,\n","                                                       inputs_geom=inputs_geom,\n","                                                       bias_geom=bias_geom,\n","                                                       c_val=c_val,\n","                                                       non_lin=cell_non_lin,\n","                                                       fix_biases=fix_biases,\n","                                                       fix_matrices=fix_matrices,\n","                                                       matrices_init_eye=matrices_init_eye,\n","                                                       dtype=dtype)\n","        elif cell_type == 'gru' and sent_geom == 'hyp':\n","            cell_class = lambda h_dim: rnn_impl.HypGRU(num_units=h_dim,\n","                                                       inputs_geom=inputs_geom,\n","                                                       bias_geom=bias_geom,\n","                                                       c_val=c_val,\n","                                                       non_lin=cell_non_lin,\n","                                                       fix_biases=fix_biases,\n","                                                       fix_matrices=fix_matrices,\n","                                                       matrices_init_eye=matrices_init_eye,\n","                                                       dtype=dtype)\n","        else:\n","            logger.error('Not valid cell type: %s and sent_geom %s' % (cell_type, sent_geom))\n","            exit()\n","\n","        # RNN 1\n","        with tf.variable_scope(cell_type + '1'):\n","            word_embeddings_1 = tf.nn.embedding_lookup(self.embeddings, self.word_ids_1) # bs x num_w_s1 x dim\n","\n","            cell_1 = cell_class(hidden_dim)\n","            initial_state_1 = cell_1.zero_state(batch_size, dtype)\n","            outputs_1, state_1 = tf.nn.dynamic_rnn(cell=cell_1,\n","                                                   inputs=word_embeddings_1,\n","                                                   dtype=dtype,\n","                                                   initial_state=initial_state_1,\n","                                                   sequence_length=self.num_words_1)\n","            if cell_type == 'TFlstm':\n","                self.sent_1 = state_1[1]\n","            else:\n","                self.sent_1 = state_1\n","\n","\n","            sent1_norm = util.tf_norm(self.sent_1)\n","\n","\n","        # RNN 2\n","        with tf.variable_scope(cell_type + '2'):\n","            word_embeddings_2 = tf.nn.embedding_lookup(self.embeddings, self.word_ids_2)\n","            # tf.summary.scalar('word_emb2', tf.reduce_mean(tf.norm(word_embeddings_2, axis=2)))\n","\n","            cell_2 = cell_class(hidden_dim)\n","            initial_state_2 = cell_2.zero_state(batch_size, dtype)\n","            outputs_2, state_2 = tf.nn.dynamic_rnn(cell=cell_2,\n","                                                   inputs=word_embeddings_2,\n","                                                   dtype=dtype,\n","                                                   initial_state=initial_state_2,\n","                                                   sequence_length=self.num_words_2)\n","            if cell_type == 'TFlstm':\n","                self.sent_2 = state_2[1]\n","            else:\n","                self.sent_2 = state_2\n","\n","\n","            sent2_norm = util.tf_norm(self.sent_2)\n","\n","\n","        tf.summary.scalar('RNN/word_emb1', tf.reduce_mean(tf.norm(word_embeddings_1, axis=2)))\n","        tf.summary.scalar('RNN/sent1', tf.reduce_mean(sent1_norm))\n","        tf.summary.scalar('RNN/sent2', tf.reduce_mean(sent2_norm))\n","\n","\n","        eucl_vars += cell_1.eucl_vars + cell_2.eucl_vars\n","        if sent_geom == 'hyp':\n","            hyp_vars += cell_1.hyp_vars + cell_2.hyp_vars\n","\n"," \n","        ## Compute d(s1, s2)\n","        if sent_geom == 'eucl':\n","            d_sq_s1_s2 = util.tf_euclid_dist_sq(self.sent_1, self.sent_2)\n","        else:\n","            d_sq_s1_s2 = util.tf_poinc_dist_sq(self.sent_1, self.sent_2, c = c_val)\n","\n","\n","        ##### Some summaries:\n","\n","        # For summaries and debugging, we need these:\n","        pos_labels = tf.reshape(tf.cast(self.label_placeholder, tf.float64), [-1, 1])\n","        neg_labels = 1. - pos_labels\n","        weights_pos_labels = pos_labels / tf.reduce_sum(pos_labels)\n","        weights_neg_labels = neg_labels / tf.reduce_sum(neg_labels)\n","\n","        ################## first feed forward layer ###################\n","\n","        # Define variables for the first feed-forward layer: W1 * s1 + W2 * s2 + b + bd * d(s1,s2)\n","        W_ff_s1 = tf.get_variable('W_ff_s1',\n","                                  dtype=dtype,\n","                                  shape=[hidden_dim, before_mlr_dim],\n","                                  initializer= tf.contrib.layers.xavier_initializer())\n","\n","        W_ff_s2 = tf.get_variable('W_ff_s2',\n","                                  dtype=dtype,\n","                                  shape=[hidden_dim, before_mlr_dim],\n","                                  initializer= tf.contrib.layers.xavier_initializer())\n","\n","        b_ff = tf.get_variable('b_ff',\n","                               dtype=dtype,\n","                               shape=[1, before_mlr_dim],\n","                               initializer=tf.constant_initializer(0.0))\n","\n","        b_ff_d = tf.get_variable('b_ff_d',\n","                                 dtype=dtype,\n","                                 shape=[1, before_mlr_dim],\n","                                 initializer=tf.constant_initializer(0.0))\n","\n","        eucl_vars += [W_ff_s1, W_ff_s2]\n","        if ffnn_geom == 'eucl' or bias_geom == 'eucl':\n","            eucl_vars += [b_ff]\n","            if additional_features == 'dsq':\n","                eucl_vars += [b_ff_d]\n","        else:\n","            hyp_vars += [b_ff]\n","            if additional_features == 'dsq':\n","                hyp_vars += [b_ff_d]\n","\n","\n","        if ffnn_geom == 'eucl' and sent_geom == 'hyp': # Sentence embeddings are Euclidean after log, except the proper distance (Eucl or hyp) is kept!\n","            self.sent_1 = util.tf_log_map_zero(self.sent_1, c_val)\n","            self.sent_2 = util.tf_log_map_zero(self.sent_2, c_val)\n","\n","        ####### Build output_ffnn #######\n","        if ffnn_geom == 'eucl':\n","            output_ffnn = tf.matmul(self.sent_1, W_ff_s1) + tf.matmul(self.sent_2, W_ff_s2) + b_ff\n","            if additional_features == 'dsq': # [u, v, d(u,v)^2]\n","                output_ffnn = output_ffnn + d_sq_s1_s2 * b_ff_d\n","\n","        else:\n","            assert sent_geom == 'hyp'\n","            ffnn_s1 = util.tf_mob_mat_mul(W_ff_s1, self.sent_1, c_val)\n","            ffnn_s2 = util.tf_mob_mat_mul(W_ff_s2, self.sent_2, c_val)\n","            output_ffnn = util.tf_mob_add(ffnn_s1, ffnn_s2, c_val)\n","\n","            hyp_b_ff = b_ff\n","            if bias_geom == 'eucl':\n","                hyp_b_ff = util.tf_exp_map_zero(b_ff, c_val)\n","            output_ffnn = util.tf_mob_add(output_ffnn, hyp_b_ff, c_val)\n","\n","            if additional_features == 'dsq': # [u, v, d(u,v)^2]\n","                hyp_b_ff_d = b_ff_d\n","                if bias_geom == 'eucl':\n","                    hyp_b_ff_d = util.tf_exp_map_zero(b_ff_d, c_val)\n","\n","                output_ffnn = util.tf_mob_add(output_ffnn,\n","                                              util.tf_mob_scalar_mul(d_sq_s1_s2, hyp_b_ff_d, c_val),\n","                                              c_val)\n","\n","        if ffnn_geom == 'eucl':\n","            output_ffnn = util.tf_eucl_non_lin(output_ffnn, non_lin=ffnn_non_lin)\n","        else:\n","            output_ffnn = util.tf_hyp_non_lin(output_ffnn,\n","                                              non_lin=ffnn_non_lin,\n","                                              hyp_output = (mlr_geom == 'hyp' and dropout == 1.0),\n","                                              c=c_val)\n","        # Mobius dropout\n","        if dropout < 1.0:\n","            # If we are here, then output_ffnn should be Euclidean.\n","            output_ffnn = tf.nn.dropout(output_ffnn, keep_prob=self.dropout_placeholder)\n","            if (mlr_geom == 'hyp'):\n","                output_ffnn = util.tf_exp_map_zero(output_ffnn, c_val)\n","\n","\n","        ################## MLR ###################\n","        # output_ffnn is batch_size x before_mlr_dim\n","\n","        A_mlr = []\n","        P_mlr = []\n","        logits_list = []\n","        for cl in range(num_classes):\n","            A_mlr.append(tf.get_variable('A_mlr' + str(cl),\n","                                         dtype=dtype,\n","                                         shape=[1, before_mlr_dim],\n","                                         initializer=tf.contrib.layers.xavier_initializer()))\n","            eucl_vars += [A_mlr[cl]]\n","\n","            P_mlr.append(tf.get_variable('P_mlr' + str(cl),\n","                                         dtype=dtype,\n","                                         shape=[1, before_mlr_dim],\n","                                         initializer=tf.constant_initializer(0.0)))\n","\n","            if mlr_geom == 'eucl':\n","                eucl_vars += [P_mlr[cl]]\n","                logits_list.append(tf.reshape(util.tf_dot(-P_mlr[cl] + output_ffnn, A_mlr[cl]), [-1]))\n","\n","            elif mlr_geom == 'hyp':\n","                hyp_vars += [P_mlr[cl]]\n","                minus_p_plus_x = util.tf_mob_add(-P_mlr[cl], output_ffnn, c_val)\n","                norm_a = util.tf_norm(A_mlr[cl])\n","                lambda_px = util.tf_lambda_x(minus_p_plus_x, c_val)\n","                px_dot_a = util.tf_dot(minus_p_plus_x, tf.nn.l2_normalize(A_mlr[cl]))\n","                logit = 2. / np.sqrt(c_val) * norm_a * tf.asinh(np.sqrt(c_val) * px_dot_a * lambda_px)\n","                logits_list.append(tf.reshape(logit, [-1]))\n","\n","        self.logits = tf.stack(logits_list, axis=1)\n","\n","        self.argmax_idx = tf.argmax(self.logits, axis=1, output_type=tf.int32)\n","\n","        self.loss = tf.reduce_mean(\n","            tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.label_placeholder,\n","                                                           logits=self.logits))\n","        tf.summary.scalar('classif/unreg_loss', self.loss)\n","\n","        if reg_beta > 0.0:\n","            assert num_classes == 2\n","            distance_regularizer = tf.reduce_mean(\n","                (tf.cast(self.label_placeholder, dtype=dtype) - 0.5) * d_sq_s1_s2)\n","\n","            self.loss = self.loss + reg_beta * distance_regularizer\n","\n","        self.acc = tf.reduce_mean(tf.to_float(tf.equal(self.argmax_idx, self.label_placeholder)))\n","        tf.summary.scalar('classif/accuracy', self.acc)\n","\n","\n","\n","\n","        ######################################## OPTIMIZATION ######################################\n","        all_updates_ops = []\n","\n","        ###### Update Euclidean parameters using Adam.\n","        optimizer_euclidean_params = tf.train.AdamOptimizer(learning_rate=1e-3)\n","        eucl_grads = optimizer_euclidean_params.compute_gradients(self.loss, eucl_vars)\n","        capped_eucl_gvs = [(tf.clip_by_norm(grad, 1.), var) for grad, var in eucl_grads]  ###### Clip gradients\n","        all_updates_ops.append(optimizer_euclidean_params.apply_gradients(capped_eucl_gvs))\n","\n","\n","        ###### Update Hyperbolic parameters, i.e. word embeddings and some biases in our case.\n","        def rsgd(v, riemannian_g, learning_rate):\n","            if hyp_opt == 'rsgd':\n","                return util.tf_exp_map_x(v, -self.burn_in_factor * learning_rate * riemannian_g, c=c_val)\n","            else:\n","                # Use approximate RSGD based on a simple retraction.\n","                updated_v = v - self.burn_in_factor * learning_rate * riemannian_g\n","                # Projection op after SGD update. Need to make sure embeddings are inside the unit ball.\n","                return util.tf_project_hyp_vecs(updated_v, c_val)\n","\n","\n","        if inputs_geom == 'hyp':\n","            grads_and_indices_hyp_words = tf.gradients(self.loss, self.embeddings)\n","            grads_hyp_words = grads_and_indices_hyp_words[0].values\n","            repeating_indices = grads_and_indices_hyp_words[0].indices\n","            unique_indices, idx_in_repeating_indices = tf.unique(repeating_indices)\n","            agg_gradients = tf.unsorted_segment_sum(grads_hyp_words,\n","                                                    idx_in_repeating_indices,\n","                                                    tf.shape(unique_indices)[0])\n","\n","            agg_gradients = tf.clip_by_norm(agg_gradients, 1.) ######## Clip gradients\n","            unique_word_emb = tf.nn.embedding_lookup(self.embeddings, unique_indices)  # no repetitions here\n","\n","            riemannian_rescaling_factor = util.riemannian_gradient_c(unique_word_emb, c=c_val)\n","            rescaled_gradient = riemannian_rescaling_factor * agg_gradients\n","\n","            all_updates_ops.append(tf.scatter_update(self.embeddings,\n","                                                     unique_indices,\n","                                                     rsgd(unique_word_emb, rescaled_gradient, lr_words))) # Updated rarely\n","\n","        if len(hyp_vars) > 0:\n","            hyp_grads = tf.gradients(self.loss, hyp_vars)\n","            capped_hyp_grads = [tf.clip_by_norm(grad, 1.) for grad in hyp_grads]  ###### Clip gradients\n","\n","\n","            for i in range(len(hyp_vars)):\n","                riemannian_rescaling_factor = util.riemannian_gradient_c(hyp_vars[i], c=c_val)\n","                rescaled_gradient = riemannian_rescaling_factor * capped_hyp_grads[i]\n","                all_updates_ops.append(tf.assign(hyp_vars[i], rsgd(hyp_vars[i], rescaled_gradient, lr_ffnn)))  # Updated frequently\n","\n","        self.all_optimizer_var_updates_op = tf.group(*all_updates_ops)\n","\n","\n","        self.summary_merged = tf.summary.merge_all()\n","        self.test_summary_writer = tf.summary.FileWriter(\n","            os.path.join(root_path, 'tb_28may/' + tensorboard_name + '/'))\n","\n","\n","    ###############################################################################################\n","    ###############################################################################################\n","    ###############################################################################################\n","    def test(self, sess, test_or_valid_data, name, summary_i):\n","        N = len(test_or_valid_data)\n","        i = 0\n","        predictions = []\n","\n","        avg_loss = 0.0\n","        num_b_in_loss = 0\n","        while i < N:\n","            batch_word_ids_1, batch_num_words_1, batch_word_ids_2, batch_num_words_2, batch_label = self.next_batch(\n","                i=i, N=N, data=test_or_valid_data)\n","\n","            if name == 'test':\n","                summary, loss, argmax_idx = \\\n","                    sess.run([self.summary_merged, self.loss, self.argmax_idx], feed_dict={\n","                        self.word_ids_1: batch_word_ids_1,\n","                        self.num_words_1: batch_num_words_1,\n","                        self.word_ids_2: batch_word_ids_2,\n","                        self.num_words_2: batch_num_words_2,\n","                        self.label_placeholder: batch_label,\n","                        self.dropout_placeholder: 1.0\n","                    })\n","                self.test_summary_writer.add_summary(summary, summary_i)\n","\n","            else:\n","                loss, argmax_idx = \\\n","                    sess.run([self.loss, self.argmax_idx], feed_dict={\n","                        self.word_ids_1: batch_word_ids_1,\n","                        self.num_words_1: batch_num_words_1,\n","                        self.word_ids_2: batch_word_ids_2,\n","                        self.num_words_2: batch_num_words_2,\n","                        self.label_placeholder: batch_label,\n","                        self.dropout_placeholder: 1.0\n","                    })\n","\n","            avg_loss += loss\n","            num_b_in_loss += 1\n","\n","            for ci in argmax_idx:\n","                predictions.append(ci)\n","\n","            i += batch_size\n","\n","        avg_loss /= num_b_in_loss\n","\n","        num_correct = 0.0\n","        glE_predE = 0.0\n","        glN_predN = 0.0\n","        glE_predN = 0.0\n","        glN_predE = 0.0\n","\n","        predictions = predictions[:N]\n","\n","        for ind, predicted_label in enumerate(predictions):\n","            gold_label = test_or_valid_data[ind][4]\n","\n","            if predicted_label == gold_label:\n","                num_correct += 1.0\n","                if predicted_label == 1:\n","                    glE_predE += 1.0\n","                else:\n","                    glN_predN += 1.0\n","            else:\n","                if predicted_label == 1:\n","                    glN_predE += 1.0\n","                else:\n","                    glE_predN += 1.0\n","\n","        accuracy = num_correct / (1.0 * N)\n","\n","        if name == 'test':\n","            logger.info('For ' + name + ': ==> ' +\n","                        ' glN_predN = ' + str(glN_predN) + '; glE_predN = ' + str(glE_predN) +\n","                        '; glN_predE = ' + str(glN_predE) + '; glE_predE = ' + str(glE_predE))\n","\n","        return accuracy\n","\n","\n","    def next_batch(self, i, N, data):\n","        it = i\n","        to = min(i + batch_size, N)\n","\n","        MAX_PAD = 0\n","        batch_word_ids_1 = []\n","        batch_num_words_1 = []\n","\n","        batch_word_ids_2 = []\n","        batch_num_words_2 = []\n","\n","        batch_label = []\n","\n","        while it < to:\n","            word_ids_1 = data[it][0]\n","            num_words_1 = data[it][1]\n","            word_ids_2 = data[it][2]\n","            num_words_2 = data[it][3]\n","            label = data[it][4]\n","\n","            MAX_PAD = max(MAX_PAD, max(len(word_ids_1), len(word_ids_2)))\n","\n","            batch_word_ids_1.append(word_ids_1)\n","            batch_num_words_1.append(num_words_1)\n","\n","            batch_word_ids_2.append(word_ids_2)\n","            batch_num_words_2.append(num_words_2)\n","\n","            batch_label.append(label)\n","\n","            it += 1\n","\n","            if it == to and to == N:\n","                it = 0\n","                to = batch_size - len(batch_word_ids_1)\n","\n","        for ind in range(0, batch_size):\n","\n","            while len(batch_word_ids_1[ind]) < MAX_PAD:\n","                batch_word_ids_1[ind].append(0)\n","\n","            while len(batch_word_ids_2[ind]) < MAX_PAD:\n","                batch_word_ids_2[ind].append(0)\n","\n","            if len(batch_word_ids_1[ind]) != MAX_PAD or len(batch_word_ids_2[ind]) != MAX_PAD:\n","                logger.error('THIS IS WRONG!: len1: %d\\nlen2: %d\\nMAX_PAD:%d\\n' %\n","                             (len(batch_word_ids_1[ind]), len(batch_word_ids_2[ind]), MAX_PAD))\n","                exit()\n","\n","        batch_word_ids_1 = np.array(batch_word_ids_1)\n","        batch_num_words_1 = np.array(batch_num_words_1)\n","\n","        batch_word_ids_2 = np.array(batch_word_ids_2)\n","        batch_num_words_2 = np.array(batch_num_words_2)\n","        batch_label = np.array(batch_label)\n","\n","        return batch_word_ids_1, batch_num_words_1, batch_word_ids_2, batch_num_words_2, batch_label\n","\n","\n","    def dataset_to_minibatches(self, dataset):\n","        N = len(dataset)\n","        i = 0\n","        batches_list = []\n","        while i < N:\n","            batches_list.append(self.next_batch(i=i, N=N, data=dataset))\n","            i += batch_size\n","        return batches_list\n","\n","\n","    def train(self, training_data, dev_data, test_data, restore_model=False, save_model=False,\n","              restore_from_path=None, save_to_path=None):\n","\n","        training_data_batches = self.dataset_to_minibatches(training_data)\n","        num_batches = len(training_data_batches)\n","        N = len(training_data)\n","\n","        saver = tf.train.Saver()\n","        best_test_accuracy = 0.0\n","        best_validation_accuracy = 0.0\n","        best_i = 0\n","\n","        PRINT_STEP = 2000\n","\n","        config = tf.ConfigProto(\n","            intra_op_parallelism_threads=5,\n","            inter_op_parallelism_threads=3,\n","            log_device_placement=True\n","        )\n","        config.gpu_options.allow_growth = True\n","        with tf.Session(config=config) as sess:\n","\n","            sess.run(tf.global_variables_initializer())\n","            in_embeds = self.embeddings.eval()\n","\n","            if restore_model:\n","                saver.restore(sess, restore_from_path)\n","            else:\n","                sess.run(tf.global_variables_initializer())\n","\n","            sess.graph.finalize()\n","\n","            trained_embeds = self.embeddings.eval()\n","\n","            return in_embeds,trained_embeds\n","\n","\n","            # Uncomment after\n","\n","        #     burn_in_factor = 1.0\n","        #     epoch = -1\n","        #     while epoch < num_epochs:\n","        #         epoch += 1\n","        #         logger.info('Epoch: %d' % epoch)\n","\n","        #         cur_total_time = 0\n","        #         shuffle(training_data_batches) # Shuffle training data after each epoch.\n","\n","        #         if burnin and epoch == 0:\n","        #             burn_in_factor /= 10.0\n","        #         i = -1\n","        #         while i < num_batches - 1:\n","        #             i += 1\n","\n","        #             # Training\n","        #             batch_word_ids_1, batch_num_words_1, \\\n","        #             batch_word_ids_2, batch_num_words_2, \\\n","        #             batch_label = training_data_batches[i]\n","\n","        #             feed_dict = {\n","        #                 self.word_ids_1: batch_word_ids_1,\n","        #                 self.num_words_1: batch_num_words_1,\n","        #                 self.word_ids_2: batch_word_ids_2,\n","        #                 self.num_words_2: batch_num_words_2,\n","        #                 self.label_placeholder: batch_label,\n","        #                 self.burn_in_factor: burn_in_factor,\n","        #                 self.dropout_placeholder: dropout\n","        #             }\n","\n","        #             sess_time_start = time.time()\n","        #             curr_loss, _ = sess.run([self.loss, self.all_optimizer_var_updates_op], feed_dict=feed_dict)\n","        #             cur_total_time += time.time() - sess_time_start\n","\n","        #             if i % PRINT_STEP == 0:\n","        #                 if i > 0:\n","        #                     avg_sec_per_sent = cur_total_time / (PRINT_STEP * batch_size)\n","        #                     logger.info('Num examples processed: %d. curr_loss: %.4f; sec_per_sent: %.4f' % (\n","        #                         epoch * N + i * batch_size, curr_loss, avg_sec_per_sent))\n","        #                 cur_total_time = 0\n","\n","\n","        #                 # Testing\n","        #                 validation_accuracy = self.test(sess,\n","        #                                                 dev_data,\n","        #                                                 'validation',\n","        #                                                 epoch * N + i * batch_size)\n","        #                 test_accuracy = self.test(sess,\n","        #                                           test_data,\n","        #                                           'test',\n","        #                                           epoch * N + i * batch_size)\n","        #                 logger.info('CURRENT val accuracy: %.4f ; test accuracy: \\033[92m %.4f \\033[0m' %\n","        #                             (validation_accuracy, test_accuracy))\n","\n","        #                 if validation_accuracy > best_validation_accuracy:\n","        #                     best_validation_accuracy = validation_accuracy\n","        #                     best_test_accuracy = test_accuracy\n","        #                     best_i = epoch * N + i * batch_size\n","\n","        #                 logger.info(('BEST: i = %d, val acc: ' + '\\033[94m' + ' %.2f' + '\\033[0m' +\n","        #                              ', test acc: ' + '\\033[91m' + ' %.2f' + '\\033[0m') %\n","        #                             (best_i, 100 * best_validation_accuracy, 100 * best_test_accuracy))\n","\n","        #                 if save_model:\n","        #                     store_time_begin = time.time()\n","        #                     saver.save(sess, '%s_epoch_%d_it_%d.ckpt' % (save_to_path, epoch, i))\n","        #                     logger.info('Stored the model in %d seconds.' %\n","        #                                 (time.time() - store_time_begin))\n","\n","        #                 logger.info('EXPERIMENT = ' + name_experiment)\n","        #                 logger.info('=============================================================')\n","\n","        #             if np.isinf(curr_loss) or np.isnan(curr_loss):\n","        #                 logger.error('At example ' + str(epoch * N + i * batch_size) +\n","        #                              '; curr_loss: ' + str(curr_loss))\n","        #                 exit()\n","\n","\n","        # logger.info(('DONE -- BEST: i = %d, val acc: ' + '\\033[94m' + ' %.2f' + '\\033[0m' +\n","        #              ', test acc: ' + '\\033[91m' + ' %.2f' + '\\033[0m') %\n","        #             (best_i, 100 * best_validation_accuracy, 100 * best_test_accuracy))\n","\n","\n","def get_datasets():\n","    logger.info('Loading train - val - test data')\n","\n","    test_data = pickle.load(open(test_data_file_path, 'rb'))\n","    dev_data = pickle.load(open(dev_data_file_path, 'rb'))\n","    training_data = pickle.load(open(training_data_file_path, 'rb'))\n","\n","    logger.info('Training data size: %d' % len(training_data))\n","    class_to_count = {1: 0.0, 0: 0.0}\n","    for i in range(len(training_data)):\n","        class_to_count[training_data[i][4]] += 1.0\n","    for cl in class_to_count:\n","        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(training_data)))\n","\n","    logger.info('Validation data size: %d' % len(dev_data))\n","    class_to_count = {1: 0.0, 0: 0.0}\n","    for i in range(len(test_data)):\n","        class_to_count[test_data[i][4]] += 1.0\n","    for cl in class_to_count:\n","        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(dev_data)))\n","\n","    logger.info('Test data size: %d' % len(test_data))\n","    class_to_count = {1: 0.0, 0: 0.0}\n","    for i in range(len(test_data)):\n","        class_to_count[test_data[i][4]] += 1.0\n","    for cl in class_to_count:\n","        logger.info('Class %d has %.4f percent samples' % (cl, 100. * class_to_count[cl] / len(test_data)))\n","\n","    return training_data, dev_data, test_data\n","\n","\n","def run():\n","    word_to_id = pickle.load(open(word_to_id_file_path, 'rb'))\n","    id_to_word = pickle.load(open(id_to_word_file_path, 'rb'))\n","\n","    model = HyperbolicRNNModel(word_to_id=word_to_id,\n","                               id_to_word=id_to_word)\n","\n","    training_data, dev_data, test_data = get_datasets()\n","\n","    in_embeds,trained_embeds = model.train(training_data=training_data,\n","                                            dev_data=dev_data,\n","                                            test_data=test_data,\n","                                            save_model=save_model,\n","                                            save_to_path=save_to_path,\n","                                            restore_model=restore_model,\n","                                            restore_from_path=restore_from_path)\n","\n","    return in_embeds,trained_embeds\n","\n","if __name__=='__main__':\n","  in_embeds,trained_embeds = run()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From <ipython-input-6-19e5ebaa6fe3>:40: calling RandomUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Call initializer instance with the dtype argument instead of passing it to the constructor\n","WARNING:tensorflow:From <ipython-input-6-19e5ebaa6fe3>:98: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n","Init RNN cell\n","WARNING:tensorflow:From /tensorflow-1.15.2/python3.6/tensorflow_core/python/ops/clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","Init RNN cell\n","WARNING:tensorflow:\n","The TensorFlow contrib module will not be included in TensorFlow 2.0.\n","For more information, please see:\n","  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n","  * https://github.com/tensorflow/addons\n","  * https://github.com/tensorflow/io (for I/O related ops)\n","If you depend on functionality not listed there, please file an issue.\n","\n","WARNING:tensorflow:From <ipython-input-6-19e5ebaa6fe3>:280: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n"],"name":"stdout"},{"output_type":"stream","text":["2020/03/30 13:32:46: Loading train - val - test data\n","2020/03/30 13:32:49: Training data size: 549367\n","2020/03/30 13:32:49: Class 1 has 33.3868 percent samples\n","2020/03/30 13:32:49: Class 0 has 66.6132 percent samples\n","2020/03/30 13:32:49: Validation data size: 9842\n","2020/03/30 13:32:49: Class 1 has 34.2207 percent samples\n","2020/03/30 13:32:49: Class 0 has 65.5964 percent samples\n","2020/03/30 13:32:49: Test data size: 9824\n","2020/03/30 13:32:49: Class 1 has 34.2834 percent samples\n","2020/03/30 13:32:49: Class 0 has 65.7166 percent samples\n"],"name":"stderr"},{"output_type":"stream","text":["Device mapping:\n","/job:localhost/replica:0/task:0/device:XLA_CPU:0 -> device: XLA_CPU device\n","/job:localhost/replica:0/task:0/device:XLA_GPU:0 -> device: XLA_GPU device\n","/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\n","\n","INFO:tensorflow:Restoring parameters from /content/gdrive/My Drive/hyperbolic_nn/2d_embedds/Rand_in_all_epochs_2/experiment_2d_hyp_rnn_rsgd_pretrain_poincare_glove_50_epoch_epoch_6_it_0.ckpt\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-YPWmd9oYAIR","colab_type":"code","outputId":"40fb84c2-4962-4692-c991-bb53a35a7091","executionInfo":{"status":"ok","timestamp":1585575178972,"user_tz":-60,"elapsed":24863,"user":{"displayName":"Seamus O'Keeffe","photoUrl":"","userId":"11106926260732909747"}},"colab":{"base_uri":"https://localhost:8080/","height":268}},"source":["\n","fig = plt.figure()\n","plt.scatter(trained_embeds[:,0], trained_embeds[:,1])\n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYIAAAD7CAYAAABnoJM0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"id":"3p3PB5MNM9wJ","colab_type":"code","colab":{}},"source":["root_path = '/content/gdrive/My Drive/hyperbolic_nn'\n","\n","# np.savetxt(root_path+\"/rand_in_hyperbolic_dim_2.txt\", in_embeds,delimiter=',')\n","np.savetxt(root_path+\"/2d_embedds/Rand_in_all_epochs/rand_in_hyperbolic_dim_2_epoch_\"+str(epoch+25)+\".txt\", trained_embeds,delimiter=',')\n"],"execution_count":0,"outputs":[]}]}